\section{Background}

An RL problem is formalized as an Markov Decision Process (MDP). A MDP is
defined by a four tuple $(\State, \Action, \Trans, \Rew)$, where $\State$ is the
state space, $\Action$ is the action space, $\Trans : \State \times \Action
\rightarrow \State$ is the system dynamics and $\Rew : \State \times \Action
\rightarrow \R $ is the reward function
In a typical RL problem the transition function $\Trans$ is not given but is
known to be static.
The objective of an RL problem is to find a policy $\policy(\state): \State
\rightarrow \Action$ that
maximize the expected cumulative
reward over time, called the returns  $R_t = \sum_{k=t}^{\infty} \rew_{k}$.
Reinforcement learning is has been typically formulated as in a single goal
context, but more recently there has been interest in the multi-goal
reinforcement learning problems.


\subsection{Single-goal Reinforcement learning}

A number of reinforcement learning algorithms use a parameteric function
estimator for the expected future reward, either a value function
$\Value(\state; \param)$ or an action-value function $\Q(\state, \act; \param)$.
The more commonly used action-value function is defined as the running estimate
of expected future reward
%
$\Q_\policy(\state, \act; \param_\Q) = \E_\policy[ \sum_{k=t}^\infty
\discount^{k-t} \Rew(\state_k, \act_k) | \state_t = \state, \act_t = \act ]$ .
%
When the policy $\policy$ is optimal, the $\Q$-function satisfies the
\emph{Bellman equation}:
%
\begin{align}
\Q^*(\state_t, \act_t; \param_\Q) &\leftarrow \Rew(\state_t, \act_t) + \discount \max_{\act \in \Action} \Q^*(\state_{t+1}, \act; \param_\Q).
                                    \label{eq:bellman}
\end{align}%
% 
Once $\Q^*$-function is approximated, a greedy policy can be computed from $\Q^*$-function easily $\policy^*(\state_t) = \arg \max_{\act \in \Action} \Q^*(\state_t, \act)$.

Recent advances in deep reinforcement learning~\citep{MnKaSiNATURE2015} view the
Bellman equation as the gradient of a loss function. For example, Deep
Q-Networks minimize the loss function whose gradient is the Bellman equation:
%
\begin{align}
  \Loss(\param_{\Q_m}, \param_{\policy_m}) = \E_{a_t\sim\policy(\state_t; \param_{\policy_m})}\left[
  \Q^*_m(\state_t, \act_t; \param_{\Q_m}) -
  \Rew(\state_t, \act_t) - \max_{\act}\discount \Q^*_t(\state_{t+1}, \act; \param_{\Q_t}) \right]^2,
  \label{eq:bellman-dqn}
\end{align}
%
where $\Q_m$ is the main network and $\Q_t$ is the target network. Target and main
networks are function approximators with same structure, but different
parameters. The target network parameters are changed slowly towards the main
network parameters either by periodically copying the main network parameters to
target network or by maintaining target network as exponential moving average of
the main network. This improves the stability of the learning.

\citet{MnKaSiNATURE2015} also introduced the idea of \emph{replay buffer}.
The transitions experienced are not used to update the neural network in order.
Instead they are stored in a replay buffer which is then sampled out of order
for independent transitions and updating the neural network.

Deep deterministic policy-gradients (DDPG)~\citep{lillicrap2015continuous}
extends DQN~\citep{MnKaSiNATURE2015} to continuous action domains by introducing
a parameterized policy function $\policy(\state; \param_\policy)$. 
Assuming the policy to be optimal, the maxima of the second term in the Bellman
equation the $\Q$-value at action chosen by the policy at that state:
%
\begin{align}
  \Loss(\param_{\Q_m}, \param_{\policy_m}) = \E_{a_t\sim\policy(\state_t; \param_{\policy_m})}\left[
  \Q^*_m(\state_t, \act_t; \param_{\Q_m}) -
  \Rew(\state_t, \act_t) -
  \discount \Q^*_t(\state_{t+1}, \policy_t(\state_{t+1}; \param_{\policy_t}); \param_{\Q_t}) \right]^2.
  \label{eq:bellman-ddpg}
\end{align}
%
The policy network is updated using policy gradients
%
\begin{align}
\nabla_{\param_{\policy_m}} \Loss \propto \frac{1}{N} \sum_t \nabla_\act \Q^*(\state_t, \act; \param_{\Q_m})\nabla_{\param_{\policy_m}} \policy(\state_t; \param_{\policy_m}).
\end{align}%
% 


\subsection{Multi-goal Reinforcement learning}
Multi-goal tasks need the specification of goal state which can change for each
trial~\citep{plappert201802multigoalrl}. The examples include navigation to a
goal location, moving a robotic arm so that the tip of the arm is at a
particular 3D location (Fetch-Reach task).

At the start of the task, a state space $\State$ (example: joint angles of the
arm), an action space $\Action$ (example: keypresses for discrete actions, joint
torques for continuous actions), and a
Goal space $\Goal$ (example: 3D coordinates of the destination location).
It is known that the transition function $\TransFull$
and the reward function $\Rew: \State \times \Action \times \Goal \rightarrow
\R$ are static.
At the start of each episode, a goal state $\goal^* \in \Goal$ is given. At each
step $t$ the agent can chose an action to take $\act_t$ and can observe
$(\state_t, \goal_t, \rew_t)$ where $\goal_t$ is the achieved goal which can be
deterministically computed from state $\goal_t = f_\goal(\state_t)$.
The reward function is formulated such that reward for reaching at the goal
$\|\goal_t - \goal^*\|_2 < \epsilon$ once
is higher than reward for visiting any other state $\infty$-times
$\Rew(\state_g, \goal^*, \act) \ge
\frac{1}{1-\discount}\max_{\state, \act'}\Rew(\state, \goal^*,
\act') \forall \act$, where $\state_g$ is such that $\|f_\goal(\state_g) -
\goal^*\| < \epsilon$.
