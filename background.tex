\section{Background}

An RL problem is formalized as a Markov Decision Process (MDP)
\citep{sutton1998reinforcement}. A MDP governs
an evolving sequence of state-action-reward triples $[(\state_0, \act_0,
\rew_0), \dots, (\state_T, \act_T, \rew_T)]$, that is full governed
by the five tuple $(\State, \Action, \Trans, \Rew, \discount)$, where $\State$ is the
state space, $\Action$ is the action space, $\Trans(\state,\act) : \State \times \Action
\rightarrow \State$ is the system dynamics, $\Rew(\state, \act) : \State \times \Action
\rightarrow \R $ is the reward function and $\discount$ is the discount
factor.
In a typical RL problem the transition function $\Trans$ is not given but is
known to be static.
In RL, the objective is to find a policy $\policy(\state): \State
\rightarrow \Action$ that
maximizes the expected cumulative
reward over time, $R_t = \sum_{k=t}^{\infty} \discount^{k-t}\rew_{k}$, called the
\emph{return}. The discount factor, $\discount < 1$, forces
the return to be finite for infinite horizons.  
Reinforcement learning is typically formulated in single goal
contexts. More recently there has been interest in multi-goal
problems
\citep{andrychowicz2017hindsight,pong2018temporal,plappert2018multi}.


\subsection{Deep Reinforcement learning}

A number of reinforcement learning algorithms use parameteric function
approximators to estimate the return in the form of 
an action-value function, $\Q(\state, \act)$:
%
\begin{align}
\Q_\policy(\state, \act) = \E_\policy\left[ \sum_{k=t}^T
  \discount^{k-t} \Rew(\state_k, \act_k)
  \middle| \state_t = \state, \act_t = \act \right] ,
  \label{eq:q-def}
\end{align}%
where $T$ is the episode length.
%
When the policy $\policy$ is optimal, the $\Q$-function satisfies the
\emph{Bellman equation}.
%which is used to learn the function
%approximator, $\Q_{*}(\state,\act; \param_\Q)$:
%
\begin{align}
    \Q_{*}(\state_t, \act_t)
  &=
    \begin{cases}
        \Rew(\state_t, \act_t) + \discount \max_{\act \in \Action}
        \Q_{*}(\state_{t+1}, \act)
      & \text{ if } t < T
      \\
      \Rew(\state_T, \act_T) & \text{ if } t = T
    \end{cases}.
  \label{eq:bellman}
\end{align}%
%
\newcommand{\Qstar}{\Q_{*}}
\newcommand{\Qtgt}{\Q_{\text{tgt}}}
\newcommand{\ytgt}{y_t}
The optimal policy can be computed from $\Q_{*}$ greedily, 
$\policy_*(\state_t) = \arg \max_{\act \in \Action} \Q_*(\state_t,
\act)$.
%\Q_*(\state_t, \act)$.
%where $\param_\Q$ are the parameters of the function approximator.  Once
%the $\Q_{*}$-function is approximated, the optimal policy is computed
%greediliy, $\policy_*(\state_t) = \arg \max_{\act \in \Action}
%\Q_*(\state_t, \act)$.
In DQN, \citet{mnih2013playing} formulate a loss function based on 
the Bellman equation to approximate the optimal $\Qstar$-function using
a deep neural network, $\Q_m$:
%
%Recent advances in deep reinforcement learning~\citep{MnKaSiNATURE2015} view the
%Bellman equation as the gradient of a loss function. For example, Deep
%Q-Networks minimize the loss function whose gradient is the Bellman equation:
%
\begin{align}
  \Loss(\param_{\Q_m}) =
    \E_{a_t\sim\policy(\state_t; \param_{\policy_m})}\left[\left(
  \Q_m(\state_t, \act_t; \param_{\Q_m}) -
  y_t  \right)^2\right],
  \label{eq:bellman-dqn}
\end{align}
where 
$\ytgt = \Rew(\state_t, \act_t) + \max_{\act}\discount \Qtgt(\state_{t+1}, \act; \param_{\Qtgt}) $
, is the \emph{target} and $\Qtgt$ is the target network. The target network is a a slower
changing copy of the main network that stabilizes learning \cite{mnih2015human}. 

% \citet{MnKaSiNATURE2015} also introduced the idea of \emph{replay buffer}.
% The transitions experienced are not used to update the neural network in order.
% Instead they are stored in a replay buffer which is then sampled out of order
% for independent transitions and updating the neural network.
In this work, we use an extension of DQN to continuous action spaces called
deep deterministic policy-gradients
(DDPG)~\citep{lillicrap2015continuous}. DDPG approximates the
policy with a policy network $\pi_{\text{tgt}}$ replacing the $\max$
operator in $\ytgt$. The target thus becomes 
$ \ytgt = \Rew(\state_t, \act_t) + \discount
\Qtgt(\state_{t+1}, \pi_{\text{tgt}}(\state_{t+1}); \param_{\Qtgt}) $.

% Assuming the policy to be optimal, the maxima of the second term in the Bellman
% equation the $\Q$-value at action chosen by the policy at that state:
% %
% \begin{align}
%   \Loss(\param_{\Q_m}, \param_{\policy_m}) = \E_{a_t\sim\policy(\state_t; \param_{\policy_m})}\left[
%   \Q^*_m(\state_t, \act_t; \param_{\Q_m}) -
%   \Rew(\state_t, \act_t) -
%   \discount \Q^*_t(\state_{t+1}, \policy_t(\state_{t+1}; \param_{\policy_t}); \param_{\Q_t}) \right]^2.
%   \label{eq:bellman-ddpg}
% \end{align}
% %
% The policy network is updated using policy gradients
% %
% \begin{align}
% \nabla_{\param_{\policy_m}} \Loss \propto \frac{1}{N} \sum_t \nabla_\act \Q^*(\state_t, \act; \param_{\Q_m})\nabla_{\param_{\policy_m}} \policy(\state_t; \param_{\policy_m}).
% \end{align}%
% % 


\subsection{Multi-goal Reinforcement learning}
Multi-goal tasks need the specification of a goal state at the start of
every episode~\citep{plappert2018multi}. Examples include navigation to a
goal location, and moving a robotic arm's end-actuator to particular 3D
locations (Fetch-Reach task).

The recent state-of-the-art algorithms for these tasks learn a goal-conditioned value
function (GCVF), $\fwargs{\state}{\act}{\goal}{}{}$, which is defined
similarly to the
$\Q$-function (Eq~\ref{eq:q-def}), but with an additional dependence of
the desired goal specification $\goal \in \Goal$ :
%
\begin{align}
\fwargs\state\act\goal\policy{} = \E_\policy\left[ \sum_{k=t}^T
  \discount^{k-t} \Rew(\state_k, \act_k, \goal)
  \middle| \state_t = \state, \act_t = \act \right] .
  \label{eq:q-def}
\end{align}%
%
The structure of the goal specification, $\goal \in \Goal$, can be
arbitrary. For example, in a robotic
arm experiment they include the desired position of the end-effector and the
desired joint angles of the robot.  To ease the learning of
correspondences between the states and the ``achieved goals'', they are
assumed to be an observable part of the Goal-MDP $[(\state_0, \act_0,
\goal_0, \rew_0), \dots, (\state_T, \act_T, \goal_T, \rew_T)]$.
Consequently, the MDP is fully governed by the six tuple $(\State,
\Action,\Goal, \Trans, \Rew, \discount)$. The reward, $\Rew(\state,\act,
\goal) : \State \times \Action\times \Goal \rightarrow \R $, and policy,
$\policy(\state, \goal): \State \times \Goal \rightarrow \Action $, are
thereby also conditioned on goal information,  


\subsubsection{Hindsight Experience Replay}
Hindsight Experience Replay (HER) \citep{andrychowicz2017hindsight}
builds upon this definition of GCVFs.  The main insight is that there is
no valuable feedback from the environment when the agent does not reach
the goal. This is further exacerbated when goal-locations are very
sparse in the state-space. 
HER solves this problem by implementing strategies that reuse these failed experiences 
to extract valuable feedback. They are implemented by recomputing their
rewards to relabel traversed states from these trajectories as desired
psuedo-goals.

In our experiments, we employ HER's \emph{future} strategy for
pseudo-goal sampling. More specifically two transitions, $t$ and $t+f$, are
sampled from the same episode in the replay memory. The goal for the
later transition $\goal_{t+f}$ is assumed to the pseudo-goal for the
first transition.  A new transition is generated for the time step $t$
with reward re-computed with $\goal_{t+f}$ as the desired goal,
$(\state_{t}, \act_t, \state_{t+1}, \Rew(\state_t, \act_t,
\goal_{t+f})$.




% % Use less math
% At the start of the task, a state space $\State$ (example: joint angles of the
% arm), an action space $\Action$ (example: keypresses for discrete actions, joint
% torques for continuous actions), and a
% Goal space $\Goal$ (example: 3D coordinates of the destination location).
% It is known that the transition function $\TransFull$
% and the reward function $\Rew: \State \times \Action \times \Goal \rightarrow
% \R$ are static.
% At the start of each episode, a goal state $\goal^* \in \Goal$ is given. At each
% step $t$ the agent can chose an action to take $\act_t$ and can observe
% $(\state_t, \goal_t, \rew_t)$ where $\goal_t$ is the achieved goal which can be
% deterministically computed from state $\goal_t = f_\goal(\state_t)$.
% The reward function is formulated such that reward for reaching at the goal
% $\|\goal_t - \goal^*\|_2 < \epsilon$ once
% is higher than reward for visiting any other state $\infty$-times
% $\Rew(\state_g, \goal^*, \act) \ge
% \frac{1}{1-\discount}\max_{\state, \act'}\Rew(\state, \goal^*,
% \act') \forall \act$, where $\state_g$ is such that $\|f_\goal(\state_g) -
% \goal^*\| < \epsilon$.
