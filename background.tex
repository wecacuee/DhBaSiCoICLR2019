\section{Background}

A reinforcement learning (RL) problem is formalized as a Markov Decision Process (MDP)
\citep{sutton1998reinforcement}. A MDP is defined by a five tuple $(\State,
\Action, \Trans, \Rew, \discount)$, that governs
a sequence of state-action-reward triples $[(\state_0, \act_0,
\rew_0), \dots, (\state_T, \act_T, \rew_T)]$.
$\State$ is the
state space, $\Action$ is the action space, $\Trans(\state,\act) : \State \times \Action
\rightarrow \State$ is the system dynamics, $\Rew(\state, \act) : \State \times \Action
\rightarrow \R $ is the reward function and $\discount$ is the discount
factor.
In a typical RL problem the transition function $\Trans$ is not given but is
known to be static.
In RL, the objective is to find a policy $\policy(\state): \State
\rightarrow \Action$ that
maximizes the expected cumulative
reward over time, $R_t = \sum_{k=t}^{\infty} \discount^{k-t}\rew_{k}$, called the
\emph{return}. The discount factor, $\discount < 1$, forces
the return to be finite for infinite horizons.  
Reinforcement learning is typically formulated in single-goal
contexts. More recently there has been interest in multi-goal
problems
\citep{andrychowicz2017hindsight,pong2018temporal,plappert2018multi}, which is
the focus of this work.


\subsection{Deep Reinforcement learning}

A number of reinforcement learning algorithms use parametric function
approximators to estimate the return in the form of 
an action-value function, $\Q(\state, \act)$:
%
\begin{align}
\Q_\policy(\state, \act) = \E_\policy\left[ \sum_{k=t}^T
  \discount^{k-t} \Rew(\state_k, \act_k)
  \middle| \state_t = \state, \act_t = \act \right] ,
  \label{eq:q-def}
\end{align}%
where $T$ is the episode length.
%
When the policy $\policy$ is optimal, the $\Q$-function satisfies the
\emph{Bellman equation} \citep{bellman1954theory}.
%which is used to learn the function
%approximator, $\Q_{*}(\state,\act; \param_\Q)$:
%
\begin{align}
    \Q_{*}(\state_t, \act_t)
  &=
    \begin{cases}
        \Rew(\state_t, \act_t) + \discount \max_{\act \in \Action}
        \Q_{*}(\state_{t+1}, \act)
      & \text{ if } t < T
      \\
      \Rew(\state_T, \act_T) & \text{ if } t = T
    \end{cases}.
  \label{eq:bellman}
\end{align}%
%
The $\Q_*()$ function can be learned using Q-learning algorithm~\citep{watkins1992qlearning}.
The optimal policy can be computed from $\Q_{*}$ greedily, 
$\policy_*(\state_t) = \arg \max_{\act \in \Action} \Q_*(\state_t,
\act)$.
%\Q_*(\state_t, \act)$.
%where $\param_\Q$ are the parameters of the function approximator.  Once
%the $\Q_{*}$-function is approximated, the optimal policy is computed
%greediliy, $\policy_*(\state_t) = \arg \max_{\act \in \Action}
%\Q_*(\state_t, \act)$.
In \emph{Deep Q-Networks} (DQN), \citet{mnih2013playing} formulate a loss function based on 
the Bellman equation to approximate the optimal $\Qstar$-function using
a deep neural network, $\Q_m$:
%
%Recent advances in deep reinforcement learning~\citep{MnKaSiNATURE2015} view the
%Bellman equation as the gradient of a loss function. For example, Deep
%Q-Networks minimize the loss function whose gradient is the Bellman equation:
%
\begin{align}
  \Loss(\param_{\Q_m}) =
    \E_{a_t\sim\policy(\state_t; \param_{\policy_m})}\left[\left(
  \Q_m(\state_t, \act_t; \param_{\Q_m}) -
  y_t  \right)^2\right],
  \label{eq:bellman-dqn}
\end{align}
where 
$\ytgt = \Rew(\state_t, \act_t) + \max_{\act}\discount \Qtgt(\state_{t+1}, \act; \param_{\Qtgt}) $
, is the \emph{target} and $\Qtgt$ is the target
network~\citep{MnKaSiNATURE2015}.
The target network is a slower-changing
copy of the main network that stabilizes learning.
\citet{MnKaSiNATURE2015} also employ \emph{replay buffers}~\citep{lin1993reinforcement}
that store transitions from episodes. During training, these transitions are
sampled out of order to train the networks in an off-policy manner, avoiding
correlation in the samples and thus leading to faster learning.
% The transitions experienced are not used to update the neural network in order.
% Instead they are stored in a replay buffer which is then sampled out of order
% for independent transitions and updating the neural network.

In this work, we use an extension of DQN to continuous action spaces called
deep deterministic policy-gradients
(DDPG)~\citep{lillicrap2015continuous}. DDPG approximates the
policy with a policy network $\policy_{\text{tgt}}(\state; \param_\policy)$ that
replaces the $\max$
operator in $\ytgt$. The target thus becomes 
$ \ytgt = \Rew(\state_t, \act_t) + \discount
\Qtgt(\state_{t+1}, \policy_{\text{tgt}}(\state_{t+1}; \param_\policy); \param_{\Qtgt})$ and the
loss function changes accordingly:
%
\newcommand{\policytgt}{\policy_\text{tgt}}%
\begin{align}
  \Loss(\param_\fw, \param_\policy) &= \E_{\act_t \sim \policy(\state_t; \param_\policy)}[
                                      (\Q_m(\state_t, \act_t; \param_\fw) - \ytgt)^2].
\end{align}
%

% Assuming the policy to be optimal, the maxima of the second term in the Bellman
% equation the $\Q$-value at action chosen by the policy at that state:
% %
% \begin{align}
%   \Loss(\param_{\Q_m}, \param_{\policy_m}) = \E_{a_t\sim\policy(\state_t; \param_{\policy_m})}\left[
%   \Q^*_m(\state_t, \act_t; \param_{\Q_m}) -
%   \Rew(\state_t, \act_t) -
%   \discount \Q^*_t(\state_{t+1}, \policy_t(\state_{t+1}; \param_{\policy_t}); \param_{\Q_t}) \right]^2.
%   \label{eq:bellman-ddpg}
% \end{align}
% %
% The policy network is updated using policy gradients
% %
% \begin{align}
% \nabla_{\param_{\policy_m}} \Loss \propto \frac{1}{N} \sum_t \nabla_\act \Q^*(\state_t, \act; \param_{\Q_m})\nabla_{\param_{\policy_m}} \policy(\state_t; \param_{\policy_m}).
% \end{align}%
% % 


\subsection{Multi-goal Reinforcement learning}
Multi-Goal Reinforcement Learning~\citep{plappert2018multi} focuses on problems
where the desired goal state can change for every episode.
State-of-the-art MGRL algorithms learn a goal-conditioned value
function (GCVF), $\fwargs{\state}{\act}{\goal}{}{}$, which is defined
similar to the $\Q$-function \eqref{eq:q-def}, but with an additional
dependence on the desired goal specification $\goal \in \Goal$ :
%
\begin{align}
\fwargs\state\act\goal\policy{} = \E_\policy\left[ \sum_{k=t}^T
  \discount^{k-t} \Rew(\state_k, \act_k, \goal)
  \middle| \state_t = \state, \act_t = \act \right] .
  \label{eq:q-def}
\end{align}%
%
The structure of the goal specification, $\goal \in \Goal$, can be
arbitrary. For example, in a robotic
arm experiment, possible goal specifications include the desired position of the
end-effector and the desired joint angles of the robot.
The states and \emph{achieved goals} are assumed to be an observable part of the
Goal-MDP to enable the agent to learn the correspondences between them,
$[(\state_0, \act_0,
\goal_0, \rew_0), \dots, (\state_T, \act_T, \goal_T, \rew_T)]$.
Consequently, this Goal-MDP is fully governed by the six tuple $(\State,
\Action,\Goal, \Trans, \Rew, \discount)$. The reward, $\Rew(\state,\act,
\goal) : \State \times \Action\times \Goal \rightarrow \R $, and policy
$\policy(\state, \goal): \State \times \Goal \rightarrow \Action $ are
also in turn conditioned on goal information.


\paragraph{Hindsight Experience Replay}
HER~\citep{andrychowicz2017hindsight}
builds upon this definition of GCVFs \eqref{eq:q-def}.  The main insight of HER is that there is
no valuable feedback from the environment when the agent does not reach
the goal. This is further exacerbated when goals are
sparse in the state-space. 
HER solves this problem by reusing these failed experiences for learning.
It recomputes a reward for each reached state by relabeling them as pseudo-goals.

In our experiments, we employ HER's \emph{future} strategy for
pseudo-goal sampling. More specifically, two transitions
from the same episode in the replay buffer for times $t$ and $t+f$ are
sampled.
The achieved goal $\goal_{t+f}$ is then assumed to be the pseudo-goal.
The algorithm generates a new transition for the time step $t$
with the reward re-computed as if $\goal_{t+f}$ was the desired goal,
$(\state_{t}, \act_t, \state_{t+1}, \Rew(\state_t, \act_t, \goal_{t+f}))$.
HER uses this new transition as a sample.



% % Use less math
% At the start of the task, a state space $\State$ (example: joint angles of the
% arm), an action space $\Action$ (example: keypresses for discrete actions, joint
% torques for continuous actions), and a
% Goal space $\Goal$ (example: 3D coordinates of the destination location).
% It is known that the transition function $\TransFull$
% and the reward function $\Rew: \State \times \Action \times \Goal \rightarrow
% \R$ are static.
% At the start of each episode, a goal state $\goal^* \in \Goal$ is given. At each
% step $t$ the agent can chose an action to take $\act_t$ and can observe
% $(\state_t, \goal_t, \rew_t)$ where $\goal_t$ is the achieved goal which can be
% deterministically computed from state $\goal_t = f_\goal(\state_t)$.
% The reward function is formulated such that reward for reaching at the goal
% $\|\goal_t - \goal^*\|_2 < \epsilon$ once
% is higher than reward for visiting any other state $\infty$-times
% $\Rew(\state_g, \goal^*, \act) \ge
% \frac{1}{1-\discount}\max_{\state, \act'}\Rew(\state, \goal^*,
% \act') \forall \act$, where $\state_g$ is such that $\|f_\goal(\state_g) -
% \goal^*\| < \epsilon$.
