\section{Background}

We present a short review of the background material that our work depends upon.

\subsection{Dijkstra}
\newcommand{\vertices}{\State}
\newcommand{\edge}{\rew}
% Floyd warshall data structure
\newcommand{\fwds}{D}
% Dijkstra data structure
\newcommand{\dds}{D}
% Q-function
Dijkstra~\citep{dijkstra1959note} is a shortest path finding algorithm from a
given vertex in the graph. Consider a weighted graph $G = (\vertices,
\edges)$, with $\vertices$ as the vertices and $\edges$ as the edges. Dijkstra
algorithms works by maintaining a data-structure $\dds : \vertices \rightarrow
\R$, that represents the shortest path length from the source. The data
structure $\dds$ is initialized with zero at start location $\dds[\state_0]
\leftarrow 0$ and a high value everywhere else $\dds[i] \leftarrow \infty \, \forall
i \in \vertices$. The algorithm then sequentially updates $\dds$ by
%
\begin{align}
  \dds[j] \leftarrow \min\{\dds[j], \edge_{(i, j)} + \dds[i] \} \, \forall (i, j) \in \edges ,
  \label{eq:dijkstra}
\end{align}%
%
where $\edge_{(i, j)}$ is the edge-weight for directed edge $(i, j) \in \edges$.
The shortest path $(\state_0, \state_{1}, \dots)$ starting from vertex
$\state_0$ can be read from $\dds$ via $\state_{t+1} = \arg \min_{i \in
\text{Nbr}(\state_t)} \dds[i]$ where $\text{Nbr}(\state_t) = \{ i | (i,
\state_t) \in \edges \} $ denotes the neighborhood of $\state_t$. With a
carefully chosen data-structure and traversal order, the Dijkstra Algorithm can
be made to run in $O(|\vertices|\log|\vertices|)$.

\subsection{Q-Learning}
Q-learning~\citep{watkins1992qlearning} is a reinforcement learning (RL)
algorithm that allows agent to explore environment and simultaneously
compute maximum reward paths.

An RL problem is formalized as an Markov Decision Process (MDP). A MDP is
defined by a four tuple $(\State, \Action, \Trans, \Rew)$, where $\State$ is the
state space, $\Action$ is the action space, $\Trans : \State \times \Action
\rightarrow \State$ is the system dynamics and $\Rew : \State
\rightarrow \R $ is the reward yielded on a execution of an action.
The objective of a typical RL problem is to maximize the expected cumulative
reward over time, called the returns  $ R_t = \sum_{t^{'}=t}^{T} \rew_{t^{'}}$.

Q-learning works by maintaining an action-value function $\Q : \State \times
\Action \rightarrow \R$ which is defined as the expected return
$\Q_\policy(\state_t, \act_t) = \E_\policy[R_t]$ from a given state-action pair.
The Q-learning algorithm works by updating the $\Q$-function using the Bellman
equation for every transition from $\state$ to $\state'$ on action $\act$
yielding reward $\rew$, 
%
\begin{align}
  \Q^*(\state, \act) &= \E_{\policy}\left[
                       \rew + \max_{\act^{'}} \Q^*(\state^{'}, \act^{'})
                       \middle| \state, \act \right] \, .
    \label{eq:q-learn-bellman}
\end{align}%
%

\subsection{Floyd-Warshall}

The Floyd-Warshall algorithm~\citep{floydwarshall1962} is a shortest path finding
algorithm from any vertex to any other vertex in a graph.
Similar to Dijkstra's algorithm, the Floyd-Warshall algorithm
finds the shortest path by keeping maintaining a shortest distance
data-structure $\fwds : \vertices \times \vertices \rightarrow \R$. between any
two pair of vertices $i, j \in \vertices$.
The data-structure $\fwds$ is initialized with edges weights
$\fwds[i, j] \leftarrow \edge_{(i, j)} \, \forall (i, j) \in \edges$
and the uninitialized edges are assigned a high value,
$\fwds[i, j] \leftarrow \infty \, \forall i, j \in \vertices$.
The algorithm works by sequentially observing all the nodes in the graph and
updating $\fwds$ with the shortest explored path known so far:
%
\begin{align}
  \fwds[i, j] \leftarrow \min\{ \fwds[i, j], \fwds[i, k] + \fwds[k, j] \} \quad
  \forall i, j, k \in \vertices \, .
\end{align}%
%


The update equation in the algorithm depends upon the triangular inequality for
shortest paths distances ($\fwds[i, j] \le \fwds[i, k] + \fwds[k, j]$) and hence
works only in the absence of negative cycles in the graph. Fortunately, many
practical problems can be formulated such that negative cycles
do not occur. The Floyd-Warshall algorithm runs in $O(|\vertices|^3)$
and is suitable for dense graphs. There also exists extensions of the algorithm
like Johnson's algorithm~\citep{johnson1977efficient} that run in
$O(|\vertices|^2\log|\vertices| + |\vertices||\edges|)$ while working on
the same principle.

% TODO: Add more math (?) that draws these parallels.
Note, the parallels between Eq.~\eqref{eq:dijkstra} and
Eq.~\eqref{eq:q-learn-bellman}. Q-learning can be thought of as the
generalization of Dijkstra's algorithm with the introduction of the action space in
the graph traversal problem. In MDP instead of choosing the next state (or
vertex in graph traversal) to go to, we can only chose an action and the next
state gets chosen by the system dynamics.
However, RL is a much more challenging problem than path planning on graphs
because there is no complete freedom on the order of traversal over state space
and there exists a trade off between exploration and exploitation. With these
parallels in mind, we extend the Floyd-Warshall algorithm to work on an MDP and
call it Floyd-Warshall Reinforcement Learning.

