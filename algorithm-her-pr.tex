\begin{algorithm}
  \tcc{By default all states are unreachable}
  Initialize networks
  $\fwargs{\state_i}{\act_i }{\goal_j; \param_{\fwcost}}{*}{}$ and
  $\policy(\state_i, \state_g; \param_{\policy})$ \;
  Copy the main network to target network
  $\fwargs{\state_i}{\act_i ;\param_{\fwcost}}{\state_j}{t}{} \leftarrow
  \fwargs{\state_i}{\act_i ; \param_{\fwcost}}{\state_j}{*}{} $ \;

  % We do not know the goal location
  Initialize replay memory $M$ \;
  \tcc{Collect experience}
  \For{$e \leftarrow 1$ \KwTo $M$}{
    Sample $\goal_e \in \Goal$ \;
    Set $t \leftarrow 0$\;
    Observe state $\state_t$ and achieved goal $\goal_t$ \;
    \For{$t \leftarrow 1$ \KwTo $\epiT$}{
      Take action $\act_{t} \leftarrow \epsilon\text{-greedy}(\policy(\state_{t}, \goal, \fw))$ \;
      Observe $\state_{t+1}, \goal_{t+1}, \rew_t$ \;
      Store $(\state_{t}, \goal_t, \act_t, \state_{t+1}, \goal_{t+1}, \rew_t; \goal_e)$ in memory $M[e]$ \;
    }
    
    \tcc{Train}
    \For{$t \leftarrow 1$ \KwTo $\epiT$}{
      % Update the transition reward
      
      HER sample batch $B = [
      (\state_{i}, \goal_i, \act_i, \state_{i+1}, \goal_{i+1}, \rew_i;
      \goal_{i+f_i}),
      \dots ,
      (\state_{b}, \goal_b, \act_b, \state_{b+1}, \goal_{b+1}, \rew_b; \goal_{b+f_b})]$ from $M$ \;
      $\Loss(\dots) = 0$ \;
      \For{$b \in B$}{
        $(\state_{b}, \goal_b, \act_b, \state_{b+1}, \goal_{b+1}, \rew_b, \goal_{b+f_b}) = B[b]$ \;
        $\Loss(\dots) += (\fwargs{\state_{b}}{\act_{b}}{\goal_{b+1}}{*}{} - \rew_b)^2$ 
        \tcc*{Step loss}
        $\Loss(\dots) += (\fwargs{\state_{b}}{\act_{b}}{\goal_{b+f_b}}{*}{} -
        \rew_b - \discount\fwargs{\state_{b+1}}{\policy_t(\state_{b+1}, \goal_{b+f_b};\param_\policy)}{\goal_{b+f_b}}{t}{})^2$
        \tcc{DDPG loss}
      }
      Update gradients for $\fw_*$ and $\policy$ using loss $\Loss(\dots)$\;
    }
  }
  \KwResult{$\policy^*(\state_k, \state_g, \fwcost)$}
  \caption{\small Path-reward reinforcement learning}
  \label{alg:floyd-warshall-deep}
\end{algorithm}


%\begin{function}
%\eIf{$\state_g = \phi$ or $\text{all}(\fwcost(\state_t, :, \state_g) = -\infty)$ }{
%  \tcc{Exploration mode}
%  $\act^* = \arg\max_{\act} Q(\state_t, \act)$\;
%}{
%  \tcc{Exploitation mode}
%  $\act^* = \arg\max_{\act} F(\state_t, \act, \state_g)$\;
%}
%
%\caption{Policy()}%$\policy^*(\state_t, \state_g, Q(., .), \fwcost(.,.,.))$}
%\label{func:policy}
%\KwRet{$\act^*$}
%\end{function}