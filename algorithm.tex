\begin{algorithm}
  \tcc{By default all states are unreachable}
  Initialize networks $\fwargs{\state_i}{\act_i ;
    \param_{\fwcost}}{\state_j}{m}{}$ and $\policy(\state_i, \state_g; \param_{\policy})$ \;
  Copy the main network to target network
  $\fwargs{\state_i}{\act_i ;\param_{\fwcost}}{\state_j}{t}{} \leftarrow
  \fwargs{\state_i}{\act_i ; \param_{\fwcost}}{\state_j}{m}{} $ \;

  % We do not know the goal location
  Initialize replay memory $M$ \;
  Input $\goal \in \Goal$ \;
  Set $t \leftarrow 0$\;
  Observe $\state_t$ \;
  \For{$t \leftarrow 1$ \KwTo $\epiT$}{
    Take action $\act_{t} \leftarrow \epsilon\text{-greedy}(\policy(\state_{t}, \goal, \fw))$ \;
    Observe $\state_{t+1}$, $\rew_t$ \;
    Store $(\state_{t}, f_\goal(\state_t), \act_t, f_\goal(\state_{t+1}), \rew_t)$ in memory $M$ \;
    % Update the transition reward
    
    HER sample batch $B = [(\state_{i}, \goal_i, \act_i, \state_{i+1}, \goal_{i+1}, \goal_{i\text{her}}, \rew_i), \dots ,
    (\state_{b}, \goal_b, \act_b, \state_{b+1}, \goal_{b+1},
    \goal_{b\text{her}}, \rew_t)]$ from $M$ \;
    Shuffle batch $B_{\text{shuffle}} \leftarrow \text{shuffle}(B)$ \;
    $\Loss(\dots) = 0$ \;
    \For{$b \in B$}{
      $(\state_{b}, \goal_b, \act_b, \state_{b+1}, \goal_{b+1}, \goal_{b\text{her}},  \rew_b) = B[b]$ \;
      $(\state_{s}, \goal_s, \act_s, \state_{s+1}, \goal_{s+1},
      \goal_{\text{her}}, \rew_s) = B_{\text{shuffle}}[b]$ \;
      $\Loss(\dots) += (\fwargs{\state_{b}}{\act_{b}}{\goal_{b+1}}{m}{} - \rew_b)^2$ 
      \tcc*{Step loss}
      $\Loss(\dots) += (\fwargs{\state_{b}}{\act_{b}}{\goal_{b\text{her}}}{m}{} -
      \rew_b + \discount\fwargs{\state_{b}}{\policy_t(\state_b, \goal_{b\text{her}};\param_\policy)}{\goal_{b\text{her}}}{t}{})^2$
      \tcc{DDPG loss}
      $\Loss(\dots) += |
      \fwargs{\state_{b}}{\act_b}{\goal_s}{t}{}
      + \fwargs{\state_{s}}{\policy_t(\state_s, \goal_{b};\param_\policy)}{\goal_b}{t}{}
      - \fwargs{\state_{b}}{\act_b}{\goal_b}{m}{}
      |_+^2$
      \tcc*{Floyd-Warshall upper bound}
      $\Loss(\dots) += |
      \fwargs{\state_{b}}{\act_b}{\goal_s}{m}{}
      + \fwargs{\state_{s}}{\policy_t(\state_s, \goal_{b};\param_\policy)}{\goal_b}{t}{}
      - \fwargs{\state_{b}}{\act_b}{\goal_b}{t}{}
      |_+^2$
      \tcc*{Floyd-Warshall lower bound}
    }
  }
  \KwResult{$\policy^*(\state_k, \state_g, \fwcost)$}
  \caption{\small Deep Floyd-Warshall Reinforcement Learning (DDPG)}
  \label{alg:floyd-warshall-deep}
\end{algorithm}


%\begin{function}
%\eIf{$\state_g = \phi$ or $\text{all}(\fwcost(\state_t, :, \state_g) = -\infty)$ }{
%  \tcc{Exploration mode}
%  $\act^* = \arg\max_{\act} Q(\state_t, \act)$\;
%}{
%  \tcc{Exploitation mode}
%  $\act^* = \arg\max_{\act} F(\state_t, \act, \state_g)$\;
%}
%
%\caption{Policy()}%$\policy^*(\state_t, \state_g, Q(., .), \fwcost(.,.,.))$}
%\label{func:policy}
%\KwRet{$\act^*$}
%\end{function}
