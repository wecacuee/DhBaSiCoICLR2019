\begin{algorithm}
  \tcc{By default all states are unreachable}
  Initialize $\fwargs{\state_i}{\act_i ; \param_{\fwcost}}{\state_j}{}{} \leftarrow -\infty$ \;
  % We do not know the goal location
  Initialize $\state_g = \phi$ \;
  Set $t \leftarrow 0$\;
  Observe $\state_t$ \;
  \For{$t \leftarrow 1$ \KwTo $\epiT$}{
    Take action $\act_{t} \sim \text{Egreedy}(\policy^*(\state_{t}, \state_g, \fw))$ \;
    Observe $\state_{t+1}$, $\rew_t$ \;
    \If{$\rew_t >= \Rgoal$}{
      \tcc{Note the goal state}
      $\state_g \leftarrow \state_t$ \;
      \tcc{Do not update the value function with goal reward}
      continue\;
    }
    % Update the transition reward
    $\fwargs{\state_{t}}{\act_{t}}{\state_{t+1}}{}{} \leftarrow \rew_t$ \;
    \For{$\state_k \in \State, \act_k \in \Act, \state_l \in \State$}{
      $\fwargs{\state_k}{\act_k}{\state_l}{}{} \leftarrow
        \max \{
        \fwargs{\state_k}{\act_k}{\state_l}{}{},
        \fwargs{\state_k}{\act_k}{\state_t}{}{}
        + \max_{\act_p \in \Act} \fwargs{\state_t}{\act_p}{\state_l}{}{}
        \}$
        \;
    }
  }
  \KwResult{$\policy^*(\state_k, \state_g, \fwcost)$}
  \caption{\small Floyd-Warshall Reinforcement Learning (Tabular setting)}
  \label{alg:floyd-warshall-small}
\end{algorithm}


%\begin{function}
%\eIf{$\state_g = \phi$ or $\text{all}(\fwcost(\state_t, :, \state_g) = -\infty)$ }{
%  \tcc{Exploration mode}
%  $\act^* = \arg\max_{\act} Q(\state_t, \act)$\;
%}{
%  \tcc{Exploitation mode}
%  $\act^* = \arg\max_{\act} F(\state_t, \act, \state_g)$\;
%}
%
%\caption{Policy()}%$\policy^*(\state_t, \state_g, Q(., .), \fwcost(.,.,.))$}
%\label{func:policy}
%\KwRet{$\act^*$}
%\end{function}
