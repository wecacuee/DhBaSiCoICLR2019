\def\year{2019}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai19}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{xcolor}
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
\setcounter{secnumdepth}{2}  

\usepackage[ruled,vlined]{algorithm2e}

\input{preamble}

% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Floyd-Warshall Reinforcement Learning: \\Combining Advantages from
Model-Free and Model-Based RL}

\author{Vikas Dhiman$^1$, Shurjo Banerjee$^1$, Jeffrey M. Siskind$^2$ and Jason J.
Corso$^1$\\
The University of Michigan$^1$\\
Purdue University$^2$}

\pdfinfo{
/Title ()
/Author ()}
\begin{document}

\maketitle
\begin{abstract}
Many practical problems in robotics, like navigation, pick and place can be
formulated as goal conditioned tasks, tasks where the environment is static but
the goal location is dynamic.
However, \emph{model-free} reinforcement learning (RL) is not suitable for these
tasks because the learned function approximations like action-value function are
not useful in transferring learned information once the reward distribution
changes.
On the other hand, \emph{model-based} reinforcement learning algorithms can
transfer to changing reward distributions by simply retaining the the
explicitly learned state-dynamics. However, model-based algorithms do not
achieve high asymptotic algorithms like model-free RL. This is because minor
modeling errors in state-dynamics cumulate over planning over long term.
To address this limitation in model-free RL on goal conditioned tasks, 
this work introduces Floyd-Warshall Reinforcement Learning (FWRL), a novel
algorithm for learning goal-conditioned value functions (GVF).
The algorithm works by constraining GVFs to be greater than the
summation of GVFs via any other path.  
Which states that the optimal FW function from a given start state to a
given goal state should be greater than or equal to the summation of FW
function via any intermediate state. 
FWRL is shown to transfer knowledge about an environment when the reward
location is dynamic compared to a model-based Q-learning baseline.
\end{abstract}


\section{ Introduction}

\input{intro}

%\input{claims}

\input{related_work}

\input{background}

\input{method}

\input{experiments}

\input{results}

\input{conclusion}

%\input{algorithm}

\input{future_work}

\begin{itemize}
    \item Grid world: Set up a random goal static maze scenario, compare with normal Q-learning.
    \item Lava world: Set up a Lava world like \cite{schaul2015universal} and test on it.
    \item \TODO{Deepmind Lab}: Set up a random goal static maze scenario, compare with normal Q-learning. 
\end{itemize}



\def\localbib{/home/dhiman/wrk/group-bib/shared}
\IfFileExists{\localbib.bib}{
\bibliography{\localbib,main,main_filtered}}{
\bibliography{main,main_filtered}}
\bibliographystyle{aaai}
\end{document}

LaTeX/MPS finished at Mon Sep  3 11:44:55
