%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{iclr2019_conference}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage{booktabs}
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
\setcounter{secnumdepth}{2}  

\usepackage[ruled,vlined]{algorithm2e}

\input{preamble}

% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
%\title{Do Goal-Conditioned Value Functions need Goal Rewards to Learn?}
\title{Learning Goal-Conditioned Value Functions without Goal Rewards}

\author{Anonymous}
%\author{Vikas Dhiman$^1$, Shurjo Banerjee$^1$, Jeffrey M. Siskind$^2$ and Jason J.
%Corso$^1$\\
%The University of Michigan$^1$\\
%Purdue University$^2$}

\pdfinfo{
/Title ()
/Author ()}
\begin{document}

\maketitle
\begin{abstract}
    Multi-goal reinforcement learning addresses the tasks where goal
    specifications are required. State-of-the-art methods in this field,
    utilize goal-conditioned value functions in their operation.
    Estimating these functions operate on the assumption that the
    achievement of goal states are tied with large reward. Our first
    contribution is the redefinition of these functions as expected
    cumulative path rewards that allows for equally efficient learning
    in the absence of these goal rewards. This formulation of the
    goal-less learning of goal-conditioned value functions obviates the
    requirement of reward-recomputation that is needed by state-of-the-art MGRL
    algorithms. This futher leads to substantially improved reward
    sample complexity which is our second contribution. Our third
    contribution is the extension of the Floyd-Warshall Reinforcement
    Learning algorithm from tabular domains to deep neural networks. 
\end{abstract}


\input{intro}

\input{background}

\input{method}

\input{experiments}

\input{results}

\input{discussion}

\input{related_work}

\input{conclusion}

%\input{algorithm}

%\input{future_work}


\def\localbib{/home/dhiman/wrk/group-bib/shared}
\IfFileExists{\localbib.bib}{
\bibliography{\localbib,main,main_filtered}}{
\bibliography{main,main_filtered}}
\bibliographystyle{iclr2019_conference}

\input{appendix}

\newpage

\section{Old Experiments}
\input{old-experiments}
\end{document}
