\def\year{2019}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai19}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{xcolor}
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
\setcounter{secnumdepth}{2}  

\usepackage[ruled,vlined]{algorithm2e}

\input{preamble}

% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Floyd-Warshall Reinforcement Learning}

%\author{Vikas Dhiman$^1$, Shurjo Banerjee$^1$, Jeffrey M. Siskind$^2$ and Jason J.
%Corso$^1$\\
%The University of Michigan$^1$\\
%Purdue University$^2$}

\pdfinfo{
/Title ()
/Author ()}
\begin{document}

\maketitle
\begin{abstract}
Many practical problems in robotics, called multi-goal tasks, involve a static
environment and dynamic goals. Example of such problems include goal-driven
navigation and pick and place tasks.
Two types of Reinforcement Learning (RL) algorithms are used for
multi-goal tasks: 
\emph{model-based} or \emph{model-free}. Each of these approaches has its limitations.
Model-free (RL) struggles to transfer learned information when the goal location
changes, but achieves high asymptotic accuracy in single goal tasks. Model-based
RL can transfer learned information to new goal locations by retaining the
explicitly learned state-dynamics, but is limited by the fact that small errors in
state-dynamics accumulate over long term planning.
In this work, we improve upon the limitations of model-free RL in
multi-task domains. 
We do this by adapting the Floyd-Warshall algorithm to
the RL domain. We call our adaptation Floyd-Warshall RL (FWRL).
The proposed algorithm learns a goal-conditioned value function. 
It works by constraining the value of
optimal path between any two states to be greater than or equal to the value of
path via any intermediate state.
We experimentally show that FWRL learns better higher reward strategies in
multi-goal tasks as compared to Q-learning and model-based RL in a
tabular domain.
\end{abstract}


\section{ Introduction}

\input{intro}

%\input{claims}

\input{related_work}

\input{background}

\input{method}

\input{experiments}

\input{results}

\input{conclusion}

%\input{algorithm}

\input{future_work}

%\begin{itemize}
%    \item Grid world: Set up a random goal static maze scenario, compare with normal Q-learning.
%    \item Lava world: Set up a Lava world like \cite{schaul2015universal} and test on it.
%    \item \TODO{Deepmind Lab}: Set up a random goal static maze scenario, compare with normal Q-learning. 
%\end{itemize}



\def\localbib{/home/dhiman/wrk/group-bib/shared}
\IfFileExists{\localbib.bib}{
\bibliography{\localbib,main,main_filtered}}{
\bibliography{main,main_filtered}}
\bibliographystyle{aaai}
\end{document}

LaTeX/MPS finished at Mon Sep  3 11:44:55
