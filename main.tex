%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{iclr2019_conference}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage{booktabs}
\usepackage{cases}
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
\setcounter{secnumdepth}{2}  

\usepackage[ruled,vlined]{algorithm2e}

\input{preamble}

% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
%\title{Do Goal-Conditioned Value Functions need Goal Rewards to Learn?}
\title{Learning Goal-Conditioned Value Functions without Goal Rewards}

\author{Anonymous}
%\author{Vikas Dhiman$^1$, Shurjo Banerjee$^1$, Jeffrey M. Siskind$^2$ and Jason J.
%Corso$^1$\\
%The University of Michigan$^1$\\
%Purdue University$^2$}

\pdfinfo{
/Title ()
/Author ()}
\begin{document}

\maketitle
\begin{abstract}
% TODO : The term goal is overused
% TODO : The flow is not there, what are the limitations. why do we do what we do?
    Multi-goal reinforcement learning (MGRL) addresses the tasks where desired goal
    state can change for every trial. State-of-the-art algorithms model these
    problems such that the reward formulation depends on the goal.
    This dependence is important to associate goal locations with high rewards,
    but it also introduces additional sampling cost in algorithms like Hindsight
    Experience Replay. Hindsight experience replay (HER) use episodes when the agent
    failed to reach the goal by re-sampling rewards as if the the reached states
    were pseudo-desired-goals.
    We propose a re-formulation of MGRL that yields to a similar algorithm, but
    removes the dependence of reward functions on goal.
    This formulation thus obviates the requirement of reward-recomputation that
    is needed by HER and its extensions.
    This futher leads to substantially improved
    sample efficiency in terms of reward samples. We also extend and evaluate
    Floyd-Warshall Reinforcement Learning algorithm from tabular domains to deep
    neural networks.
\end{abstract}


\input{intro}

\input{background}

\input{method}

\input{experiments}

\input{results}

\input{discussion}

\input{related_work}

\input{conclusion}

%\input{algorithm}

%\input{future_work}


\def\localbib{/home/dhiman/wrk/group-bib/shared}
\IfFileExists{\localbib.bib}{
\bibliography{\localbib,main,main_filtered}}{
\bibliography{main,main_filtered}}
\bibliographystyle{iclr2019_conference}

\input{appendix}

\newpage

\section{Old Experiments}
\input{old-experiments}
\end{document}
