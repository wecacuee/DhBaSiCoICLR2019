\def\year{2019}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai18}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
  \pdfinfo{
/Title (2018 Formatting Instructions for Authors Using LaTeX)
/Author (AAAI Press Staff)}
\setcounter{secnumdepth}{0}  

\usepackage[ruled,vlined]{algorithm2e}

\def\state{s}
\def\statet{\state_t}
\def\statetp{\state_{t-1}}
\def\statehist{\state_{1:t-1}}
\def\statetn{\state_{t+1}}
\def\obs{\meas}
\def\obst{\obs_t}
\def\act{a}
\def\actt{\act_t}
\def\acttp{\act_{t-1}}
\def\acttn{\act_{t+1}}
\def\Obs{\mathcal{O}}
\def\ObsEnc{\Phi_o}
\def\ObsProb{P_o}
\def\ObsFunc{C}
\def\ObsFuncFull{\ObsFunc(\statet, \actt) \rightarrow \obst}
\def\ObsFuncInv{\ObsFunc^{-1}}
\def\ObsFuncInvFull{\ObsFuncInv(\obst, \statetp, \actt) \rightarrow \statet}
\def\State{\mathcal{S}}
\def\Action{\mathcal{A}}
\def\TransP{P_{T}}
\def\Trans{T}
\def\TransFull{\Trans(\statet, \actt) \rightarrow \statetn}
\def\TransObs{T_c}
\def\Rew{R}
\def\rew{r}
\def\rewards{\vect{r}_{1:t}}
\def\rewt{\rew_t}
\def\rewtp{\rew_{t-1}}
\def\rewtn{\rew_{t+1}}
\def\RewFull{\Rew(\statet, \actt) \rightarrow \rewtn}
\def\TransObsFull{\TransObs(\statet, \obst, \actt, \rewt; \theta_T) \rightarrow \statetn}
\def\Value{V}
\def\pit{\pi_t}
\def\piDef{\pi(\acttn|\statet, \obst, \actt, \rewt; \theta_\pi) \rightarrow \pit(\acttn ; \theta_\pi)}
\def\Valuet{\Value_t}
\def\ValueDef{\Value(\statet, \obst, \actt, \rewt; \theta_\Value) \rightarrow \Valuet(\theta_\Value)}
\def\R{\mathbb{R}}
\def\E{\mathbb{E}}
\newcommand{\meas}{z}
\newcommand{\measurements}{\vect{\meas}_{1:t}}
\newcommand{\meast}[1][t]{\meas_{#1}}
\newcommand{\param}{\theta}
\newcommand{\policy}{\chi}
\newcommand{\graph}{G}
\newcommand{\vtces}{V}
\newcommand{\edges}{E}
\newcommand{\state}[2]{\mathbf{s}^{#1}(#2)}
\newcommand{\egos}[1][t]{\state{c}{#1}}
\newcommand{\st}{\state}
\newcommand{\stn}{\st_{t+1}}
\newcommand{\stt}{\st_t}
\newcommand{\stk}{\st_k}
\newcommand{\stj}{\st_j}
\newcommand{\sti}{\st_i}
\newcommand{\St}{\mathcal{S}}
\newcommand{\Act}{\mathcal{A}}
\newcommand{\acti}{\act_i}
\newcommand{\actt}{\act_t}
\newcommand{\lpt}{\delta}
\newcommand{\trans}{P_T}
\newcommand{\Q}{\qValue}
\newcommand{\V}{V}
\newcommand{\fw}{\fwcost}

\newcommand{\fwcost}{F}
\newcommand{\qValue}{Q}
\newcommand{\vma}{\alpha_\Value}
\newcommand{\qma}{\alpha_\qValue}
\newcommand{\prewma}{\alpha_\prew}
\newcommand{\fwma}{\alpha_\fwcost}
\newcommand{\maxValueBeam}{\vect{\state}_{\Value:\text{max}(m)}}
\newcommand{\nil}{\emptyset}
\newcommand{\discount}{\gamma}
\newcommand{\minedgecost}{\fwcost_0}
\begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Floyd-Warshall Deep Reinforcement Learning}
\author{}
\maketitle
\begin{abstract}
Problem: Multi-goal navigation without mapping
\\
Model free deep reinforcement learning is to learn $\policy(\act | \st)$ or $\Q(\st, \act)$
\\
Model based deep reinforcement learning is to learn $\trans(\stn| \stt, \actt)$ and $\V(\st)$.
and planning on a graph to get the shortest path.
Note that $\trans(.)$ is highly sparse and keeping a list of non-zero $\stn$ is much better than
keeping all the value of $\trans(.)$ for all $\stn \in \St$.
\\
Floyd-Warshall deep reinforcement learning is to generalize model based DRL to
directly learn the $\fw(\stj|\sti, \acti)$ which is the cost of reaching state $\stj$
starting from $\sti$ when the first action taken is $\acti$.
If we directly try to learn $\fw(.)$ we are likely to get conflicting results
that do not obey FW identity
$\fw(\stj|\sti, \acti) = \min_{\stk} \min_\act \fw(\stj|\stk, \act) + \fw(\stk | \sti, \acti)$
It is expected that since we will be visiting nearby states more often, so the
$\fw(.)$ will be consistent over small distances but will grow inconsistent over
large distances.
We can draw few samples from $\fw(.)$ to check for it's inconsistencies and then 
plan over graph over higher ranges.
\end{abstract}


\section{ Introduction}

The navigation problem in mobile robotics, oftentimes encompassed as the
fields of localization, mapping and path-planning, has seen significant
progress in recent decades with the rise of autonomous cars almost upon
us. Traditionally, tasks of navigation are split up in to two main
steps: \emph{exploration}, wherein the mobile robot explores an
environment and builds up a map-like data structure of that environment
and \emph{exploitation}, where the robot performs some kind of a task
efficiently while exploiting information from the previously stored
data-structure. While massively succesful, in the current era of
navigation based mobile robots, both the steps of \emph{exploration} and
\emph{exploitation} require significant environment-specific
hand-engineered design to work reliably.

A different approach has arisen as an offshoot of the recently massively
succesful paradigm of deep reinforcement learning (DRL). In DRL for
navigation, the completion of the navigation-based objective task within
an environment is grounded with a positive reward. To achieve this
reward, DRL agents have been shown to be able to learn to chain together
navigation derived local actions so as to best maximize the
reinforcement learning objective.  This arisal of navigational abilities
via the long-term planning based chaining together of actions that arise
from agents trained via DRL has several potentional benefits. In their
ultimate form, agents thus trained may perhaps more efficiently utilize
environmental information to perform navigational tasks without the need
for complicated, environment-specific map-like data-structures that
dominates most of the navigation based mobile robotics literature.

Unfortunately, the very absence of any form of \emph{exploration} and
\emph{exploitation} has been shown to be detrimental to navigational
abilities of DRL agents when compared metrically with the abilities of
traditional SLAM derived agents.  \cite{dhiman2018critical}. Inspired by
the need to integrate these steps in a differentiable manner within the
workings of these DRL agents, we introduce a mechanism for integrating
the Floyd-Warshall algorithm within the learning of the state-value
function of a DRL agent. 


Navigation is the problem of finding shortest path from one point to another.
Multi-goal navigation is the problem of reaching a sequence of goals in a specified order.
Multi-goal navigation requires one to remember the map to accomplish the goal faster.
\textbf{Mention dataset specific examples here such as recent work in
following natural language instructions via DRL in hyper-realistic
environments etc}
If we include exploration as a part of navigation, then we need to explore what paths are
available from one point to another and we also need to find the shortest path among those
paths and the paths formed by joining those paths.
The space can be thought of an unexplored graph with nodes as the states $\st \in \St$ and
transition probabilities $P(\st_{t+1} | \st_t, a_t)$ as weighted edges.
The problem of constructing a representation of the map is building a representation that
explores the connectivity between different states by taking different actions.
How much does change the shortest path from any point to any other point?
Do we need to capture the details that we are within visible region from a point?
Assuming that we have a closed loop system at the time of execution, we can execute shortest
path planning at a higher detail in the visible area.
We only need to capture the shortest path on a resolution coarser than visible area.
How much area is visible are when there are no walls?
Dependent upon the size of the robot, we need to know the
minimum size of obstacle that will obstruct the robot.
Depending upon the resolution of obstacle detection, what is the maximum distance at which we
can detect the minimum sized obstacle that will obstruct the robot.
Let us call this distance as local planning threshold $\lpt$.
In an open space there should be a node at least in a radius of $\lpt$.

In our experiments, we showcase how our modifications (FWDRL) improves
the abilities of agents across a variety of navigation based tasks
ranging from simple goal-directed navigation in video-game environments
to natural language instruction following in hyper-realistic ones. We
open source all our code on github.  

\subsection{Claims}
\begin{itemize} \item
Using Floyd Warshall value function leads to better generalization in case of static maps and
random goals.
\item
Hypothesis: Multi-goal navigation is more common than we think. Does FW algo improves performance in attari games.
\end{itemize}

\section{Related work}
\subsection{Navigation with mapping}
 (1) CMP from Saurabh Gupta: is metric, might not working in continuous spaces.
 (2) Semi-parameteric Topological mapping: is not end to end.
 (3) Neural Map: Is actually not mapping

\subsection{Model free DRL }
does not generalize to multi-goal environments.

\subsection{Model based DRL}
Needs more exploration.
Find the paper that shows that Model based DRL can actually compete with Model free DRL as long as it models uncertainty.

\subsection{Multi-goal navigation based papers}
Mirowski 2017, 2018: No one shot map learning, does not generalizes to new maps.


\section{Method}

See Alg~\ref{alg:floyd-warshall-small}
Over simplified. Ignoring the cost of going through the entire state space.
    \begin{algorithm}
      \KwData{Graph $\graph_0 = (\vtces, \edges)$\;}
      Initialize $\fwcost(\state_i, \act_i, \state_j; \param_{\fwcost}) = 100$ \;
      Initialize $\qValue(\state_i,\act_i; \param_{\qValue}) = 1$ \;
      Initialize $\vma = 0.1$, $\prewma = 0.9$ \;
      Let minimum path cost $\minedgecost = 0.05$ \;
      Observe $\meas_0$ from environment \;
      $\state_0 = \ObsEnc(\meas_0; \param_E)$ \;
      \For{$t \leftarrow 1$ \KwTo $\epiT$}{
        Take action $\act_{t-1}$\;
        Observe $\meas_t$, $\rew_t$\;
        Encode state $\state_t = \ObsEnc(\measurements; \param_E)$\;
        \tcc{Initialize new FW values}
        $\fwcost(\state, \act, \state_{t})
        = \min \{
               \fwcost(\state, \act, \state_t),
                \fwcost(\state, \act, \state_{t-1}) + \minedgecost
            \}
        \qquad \forall \state \in \State, \act \in \Action$ \;
        \tcc{Q-Value update}
        $\qValue(\state_{t-1}, \act_{t-1}) = (1-\qma) (\rew_t + \discount \max_{\act_k}\qValue(\state_t, \act_k)) + \qma \qValue(\state_{t-1}, \act_{t-1})$\;
        \If{$\state_t$ is visited the first time}{
            \For{$(\state_i, \state_k, \act_k) \in (\State \times \State \times \Action)$}{
                \tcc{Run the Floyd Warshall update}
                $\fwcost(\state_k, \act_k, \state_i) =
                \min \{
                    \fwcost(\state_k, \act_k, \state_i),
                    \fwcost(\state_k, \act_k, \state_t) + \min_{\act \in \Action}\fwcost(\state_t, \act, \state_i)
                \}$
                \;

                $\qValue(\state_k, \act_k) = \max \{
                        \qValue(\state_k, \act_k),
                        \max_{\act} \qValue(\state_i, \act) - \fwcost(\state_k, \act_k, \state_i)
                        \}$
                    \;

            }
        }
      }
      \KwResult{To follow the shortest path $\state_i$ to $\state_j$, follow the
        neighbors with highest $\qValue$\;
        $\policy(\state_k) = \arg \max_{\act_k \in \Action} \qValue(\state_k, \act_k)$\;
      }
      \caption{\small How to solve small windy grid world with randomized goals?}
      \label{alg:floyd-warshall-small}
  \end{algorithm}

\section{Experiments}
\begin{itemize}\item
Grid world: Set up a random goal static maze scenario, compare with normal Q-learning.
\item
Deepmind Lab: Set up a random goal static maze scenario, compare with normal Q-learning. 
\item
Atari games: Compare performance with normal Q-learning.
Analyze games in which FW does better.
Show that those games have dynamic goals rather than static.

\bibliography{main_filtered,main}
\bibliographystyle{aaai}
\end{document}
