\def\year{2019}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai18}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{natbib}
\usepackage{xcolor}
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
\setcounter{secnumdepth}{2}  

\usepackage[ruled,vlined]{algorithm2e}

\input{preamble}

%\newcommand{\TODO}[1]{{\color{red}TODO: {#1}}}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Floyd-Warshall Reinforcement Learning: \\Combining Advantages from
Model-Free and Model-Based RL \\ Improving Performance in Multi-Goal
Environments}
\author{Vikas Dhiman, Shurjo Banerjee, Jeffrey M. Siskind and Jason J.
Corso}
\pdfinfo{
/Title ()
/Author ()}
\begin{document}

\maketitle
\begin{abstract}
Reinforcement learning algorithms are oftentimes classified as either
\emph{model-free} or \emph{model-based} based on whether a state-dynamics
is learned. In the \emph{model-free} case, algorithms such as
Q-learning and policy gradients learn the optimal action value function that
maximizes expected future reward for all state-action pairs perceived by an
agent in an environment.
% VD: Not sure about that
% The reward distribution and state dynamics that governs the agent's
% environment are implicitly memorized in the learning of this function.
The action value function depends upon both reward distribution and state
dynamics but neither can be recovered from the learned action value function.
That is why, \emph{model-free} RL struggles with transferring learned behaviours
to tasks where only the reward distribution can change, for example,
goal-conditioned tasks.
In contrast, \emph{model-based} RL explicitly learns the state-dynamics
function allowing for the transfer of an agent's learned behaviours to
environments in which only the reward distribution changes. Oftentimes,
this explicit modelling requires  an additional planning step to predict
state-dynamics making policy computation in such algorithms
an expensive process.  Inspired by both these paradigms and the
Floyd-Warshall algorithm for path-planning on graphs,  this work
introduced Floyd-Warshall Reinforcement Learning (FWRL), a novel
algorithm that combines advantages from both \emph{model-based} and
\emph{model-free} approaches.  The algorithm works by learning a
goal-conditioned value function that transitions from model-based
behavior to model-free behavior in well explored regions of an agent's
state space.  FWRL is shown to transfer knowledge about an environment
when the reward location is dynamic compared to a model-based Q-learning
baseline.  FWRL is shown to meet the ground between model-free and
model-based algorithms by being model-free in the more frequently
visited regions while being model-based on less visited paths.
\end{abstract}


\section{ Introduction}

\input{intro}
\newpage\phantom{blabla}

\input{claims}
\newpage\phantom{blabla}

\input{related_work}
\newpage\phantom{blabla}

\input{background}
\newpage\phantom{blabla}

\input{method}
\newpage\phantom{blabla}

\input{experiments}
\newpage\phantom{blabla}

\input{results}
\newpage\phantom{blabla}

\input{conclusion}
\newpage\phantom{blabla}

%\input{algorithm}
%\newpage\phantom{blabla}

\input{future_work}
\newpage\phantom{blabla}

\begin{itemize}
    \item Grid world: Set up a random goal static maze scenario, compare with normal Q-learning.
    \item Lava world: Set up a Lava world like \cite{schaul2015universal} and test on it.
    \item \TODO{Deepmind Lab}: Set up a random goal static maze scenario, compare with normal Q-learning. 
\end{itemize}



\def\localbib{/home/dhiman/wrk/group-bib/shared}
\IfFileExists{\localbib.bib}{
\bibliography{\localbib,main,main_filtered}}{
\bibliography{main,main_filtered}}
\bibliographystyle{aaai}
\end{document}
