\def\year{2019}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai18}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{natbib}
\usepackage{xcolor}
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
\setcounter{secnumdepth}{0}  

\usepackage[ruled,vlined]{algorithm2e}
\newcommand{\TODO}[1]{{\color{red}TODO: {#1}}}

\def\state{s}
\def\statet{\state_t}
\def\statetp{\state_{t-1}}
\def\statehist{\state_{1:t-1}}
\def\statetn{\state_{t+1}}
\def\obs{\meas}
\def\obst{\obs_t}
\def\act{a}
\def\actt{\act_t}
\def\acttp{\act_{t-1}}
\def\acttn{\act_{t+1}}
\def\Obs{\mathcal{O}}
\def\ObsEnc{\Phi_o}
\def\ObsProb{P_o}
\def\ObsFunc{C}
\def\ObsFuncFull{\ObsFunc(\statet, \actt) \rightarrow \obst}
\def\ObsFuncInv{\ObsFunc^{-1}}
\def\ObsFuncInvFull{\ObsFuncInv(\obst, \statetp, \actt) \rightarrow \statet}
\def\State{\mathcal{S}}
\def\Action{\mathcal{A}}
\def\TransP{P_{T}}
\def\Trans{T}
\def\TransFull{\Trans(\statet, \actt) \rightarrow \statetn}
\def\TransObs{T_c}
\def\Rew{R}
\def\rew{r}
\newcommand{\vect}[1]{\mathbf{#1}}
\def\rewards{\vect{r}_{1:t}}
\def\rewt{\rew_t}
\def\rewtp{\rew_{t-1}}
\def\rewtn{\rew_{t+1}}
\def\RewFull{\Rew(\statet, \actt) \rightarrow \rewtn}
\def\TransObsFull{\TransObs(\statet, \obst, \actt, \rewt; \theta_T) \rightarrow \statetn}
\def\Value{V}
\def\pit{\pi_t}
\def\piDef{\pi(\acttn|\statet, \obst, \actt, \rewt; \theta_\pi) \rightarrow \pit(\acttn ; \theta_\pi)}
\def\Valuet{\Value_t}
\def\ValueDef{\Value(\statet, \obst, \actt, \rewt; \theta_\Value) \rightarrow \Valuet(\theta_\Value)}
\def\R{\mathbb{R}}
\def\E{\mathbb{E}}
\newcommand{\meas}{z}
\newcommand{\measurements}{\vect{\meas}_{1:t}}
\newcommand{\meast}[1][t]{\meas_{#1}}
\newcommand{\param}{\theta}
\newcommand{\policy}{\chi}
\newcommand{\graph}{G}
\newcommand{\vtces}{V}
\newcommand{\edges}{E}
\newcommand{\st}{\state}
\newcommand{\stn}{\st_{t+1}}
\newcommand{\stt}{\st_t}
\newcommand{\stk}{\st_k}
\newcommand{\stj}{\st_j}
\newcommand{\sti}{\st_i}
\newcommand{\St}{\mathcal{S}}
\newcommand{\Act}{\mathcal{A}}
\newcommand{\acti}{\act_i}
\newcommand{\lpt}{\delta}
\newcommand{\trans}{P_T}
\newcommand{\Q}{\qValue}
\newcommand{\V}{V}
\newcommand{\fw}{\fwcost}

\newcommand{\fwcost}{F}
\newcommand{\qValue}{Q}
\newcommand{\prew}{\Upsilon}
\newcommand{\epiT}{T}
\newcommand{\vma}{\alpha_\Value}
\newcommand{\qma}{\alpha_\qValue}
\newcommand{\prewma}{\alpha_\prew}
\newcommand{\fwma}{\alpha_\fwcost}
\newcommand{\maxValueBeam}{\vect{\state}_{\Value:\text{max}(m)}}
\newcommand{\nil}{\emptyset}
\newcommand{\discount}{\gamma}
\newcommand{\minedgecost}{\fwcost_0}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Floyd-Warshall Reinforcement Learning: \\Combining advantages from
model-free and model-based RL}
\author{Vikas Dhiman}
\pdfinfo{
/Title ()
/Author ()}
\begin{document}

\maketitle
\begin{abstract}
Reinforcement Learning (RL) algorithms are often discriminated on the
basis of whether a state-transition model is learned implicitly
(\emph{model-free}) or explicitly (\emph{model-based}). In the implicit
case i.e.\ \emph{model-free} RL, algorithms such as Q-learning and
policy gradients learn a joint state-action function that maps
state-action pairs to expected future reward in an agent's environment.
The reward distribution that governs an agent's environment is thus
implicity memorized in the learning of this function.  Due to this
combined learning of state-transition dynamics and the reward
distribution, learned behaviors in \emph{model-free} RL are hard to
transfer to environments where the state-transition model remains the
same while the reward-distribution changes.  In contrast,
\emph{model-based} RL explicity learns the state transition function
allowing for the decoupling of learned behaviours to environments in
which only the reward distribution changes. Oftentimes, this explicit
modelling leads to the requirement of an additional planning step to
compute model dynamics which can make policy computation in such
algorithms expensive.  Inspired by both these RL paradigms and the
Floyd-Warshall algorithm for path-planning on graphs, we propose
Floyd-Warshall Reinforcement Learning (FWRL), a novel algorithm that
combines advantages from both \emph{model-based} and \emph{model-free}
approaches. Our main contribution is an algorithm that learns a
goal-conditioned value function which transitions from model-based
behavior to model-free behavior in well explored regions of an agent's
state space.  We show that FWRL transfers knowledge about the
environment when the reward location is dynamic compared to a
model-based Q-learning baseline.  We show (\TODO{fill in the results
when exp done}) that FWRL meets the ground between model-free and
model-based algorithms by being model-free in the more frequently
visited regions while being model-based on less visited paths.

%The Reinforcement learning algorithms are classified on the basis of whether
%the state-transition model is learned explicitly into \emph{model-based}
%and \emph{model-free} algorithms.
%%
%The \emph{model-free} algorithms like Q-learning and policy gradients are easier to learn in cases when model is more complex than the policy.
%However, model-free algorithms are harder to transfer in cases when the reward function changes while the state-transition model remains the same.
%The \emph{model-based} algorithms learn the state-transition model explicitly hence making it
%easier to separate the environment transition from the reward distribution.
%However, model-based algorithms require an additional planning step on the model dynamics
%which make it expensive to compute the policy.
%%
%We propose a novel algorithm that is based on a combination of model-free and model-based approaches.
%Due to it's similarity to Floyd-Warshall algorithm in path-planning on graphs, we call
%it Floyd-Warshall reinforcement learning (FWRL).
%We show that FWRL transfers the knowledge about the environment when the reward location is dynamic as compared to the model-based Q-learning algorithm.
%We show (\TODO{fill in the results when exp done}) that FWRL meets the ground between model-free and model-based algorithms by being model-free in the more frequently visited regions while being model-based in less visited paths.
\end{abstract}


\section{ Introduction}

The Reinforcement learning algorithms are classified on the basis of whether
the state-transition model is learned explicitly into \emph{model-based}
and \emph{model-free} algorithms.
%
The \emph{model-free} algorithms like Q-learning and policy gradients are easier to learn in cases when model is more complex than the policy.
However, model-free algorithms are harder to transfer in cases when the reward function changes while the state-transition model remains the same.
One example of such a problem is traveling salesman problem.
Although classical traveling salesman problem is a posed as a planning problem where the space has already been explored, however it is not hard to recognize that someone has to create a map of the map through exploration and sometimes it has to be the agent them-self.
In such a scenario, the traveling salesman has a dynamic reward with a static map that needs to be
explored.
The model-free approach fails to generalize to new rewards in these scenarios, because of the conflated representation of environment model and resulting reward.
In spite of these limitations, model-free approaches have been applied to multi-goal navigation
scenarios with considerable success \cite{mirowski2018learning}.
However, all these methods depend upon exploring the entire space being a candidate goal space.
Not only model-free approaches are sample inefficient, multi-goal navigation problem exaggerates this problem.

The \emph{model-based} algorithms learn the state-transition model explicitly hence making it
easier to separate the environment transition from the reward distribution. Also model-based
algorithms are known to be sample efficient than model-free algorithms especially in cases when
the state-transition model is translation invariant in the environment. 
However, model-based algorithms require an additional planning step on the model dynamics
which make it expensive to compute the policy on the fly.
Although model-based algorithms fail to find an optimal policy, because small errors in models can add up and lead to wrong prediction over larger state spaces. 

Taking inspiration from the Floyd-Warshall algorithm in graph-based path planning domain,
we propose Floyd-Warshall reinforcement learning (FWRL) algorithm that combines the
benefits of both model-based and model-free methods.
We define the Floyd-Warshall function $F$ for a goal state $\state_j$ starting from a
state-action pair $\state_i$, $\act_i$
as the expected reward through all possible paths $p_{ij}$ between the starting and
destination state.
%
\begin{multline}
F(\state_j | \state_i, \act_k) \\
= \E_{p_{ij} \sim \pi}\left[
\sum_{\state, \act \in p_{ij} } \discount^{t} r(\state, \act) \right| p_{ij} = (\state_i, \act_k, \dots, \state_J) \Biggr] \, .
\end{multline}
%
For an optimal policy
\begin{align}
F^*(\state_i, \act_i, \state_j) =
\max_{p_{ij}} \sum_{k \in p_{ij} } \discount^{t} r(\state_k, \act_k) .
\end{align}

In terms of Q-function and Value function the Floyd-Warshall function has the following relationships
\begin{align}
V(\state_i) &= \E_{\act}[Q(\state_i, \act)|\state_i] \\
F(\state_i, \act_i, \state_j) &= V(\state_j) - Q(\state_i, \act_i).
\end{align}
For the optimal policy the expectation is replaced by the max operator, that is $F^*(\state_i, \act_i, \state_j) = \max_{\act} Q^*(\state_j, \act) - Q^*(\state_i, \act_i)$.

Due to this formulation, the optimal Floyd-Warshall function should satisfy the transitive property
\begin{align}
F^*(\state_i, \act_i, \state_j) = \max_{\state_k} \left[ F^*(\state_i, \act_i, \state_k)
+ \max_{\act_k}F^*(\state_k, \act_k, \state_j) \right]
\label{eq:transitive-fw}
\end{align}

The advantage of this formulation is that as soon as the goal state $\state_g$, the Q-function and
hence the policy can be estimated from $Q(\state, \act) = F(\state, \act, \state_g) + \Rew(\state_g)$.
Even if the path between two states has not been ever traversed, the value function
can be computed using the transitive property in Eq~\ref{eq:transitive-fw}. 

In our experiments, we show how our approach is better than both model-free Q-learning and model-based approaches. We also show how our approach can be switch between either approaches depending upon traversal experience in the state space.

\subsection{Claims}
\begin{itemize} \item
Using FWRL is better than model-free learning like Q-learning in terms of generalizing to dynamic goals, static maps.
\item 
Using FWRL is better than model-based learning because of less accumulation over long term, hence more accurate value functions.
\item FWRL is better than Hindsight Experience Replay (HER) \cite{andrychowicz2016learning} because it is a parametric representation of hindsight.
\item FWRL is better than TDM~\cite{pong2018temporal} because it does not require the distance reward function.
\end{itemize}

\section{Related work}
\subsection{Goal-conditioned value functions}
The idea of goal-conditioned value function is not new but has got attention because of revival of reinforcement learning based on deep neural networks.
We build upon the goal-conditioned value functions of \citet{schaul2015universal}.
\citet{schaul2015universal} proposed an architecture and a matrix factorization based algorithm for faster learning of UVFA (Universal value function approximators).
UVFA focused on fast estimation of goal-conditioned value functions using sparse
matrix factorization but not on bridging the gap between model-based and model-free
algorithms.


\citet{andrychowicz2016learning} introduced the idea of Hindsight experience replay (HER) two learn about the model from previous episodes even when the goal location has changed or not been achieved.
Our method can be seen as parametric approximation of ``Hindsight memory'',
that can help compress information from previous episodes instead of maintaining the
entire history of replay memory.

\cite{pong2018temporal} propose temporal difference models that estimate goal directed Q function for a specific kind of reward function, in particular the distance from the goal and in contrast with limited temporal horizon.

\subsection{Combining model-based and model-free methods}




\subsection{Navigation with mapping}
 (1) CMP from Saurabh Gupta: is metric, might not working in continuous spaces.
 (2) Semi-parameteric Topological mapping: is not end to end.
 (3) Neural Map: Is actually not mapping

\subsection{Model free DRL }
does not generalize to multi-goal environments.

\subsection{Model based DRL}
Needs more exploration.
Find the paper that shows that Model based DRL can actually compete with Model free DRL as long as it models uncertainty.

\subsection{Multi-goal navigation based papers}
Mirowski 2017, 2018: No one shot map learning, does not generalizes to new maps.


\section{Method}

See Alg~\ref{alg:floyd-warshall-small}

\subsection{Future work}
Items to improve the algorithm:
\begin{itemize} \item
\TODO{Justify the computational cost of constraint} The cost of going through the entire state space.
How do you extend to a network? and large state spaces.
\begin{enumerate}\item
Observation 1: If there is only one goal, then the computation should not be any more than Q-learning.
This can be accomplished by assuming that transitivity is satisfied till
$\state_{t-1}$ and needs to be extend to only the next step. This sounds similar to the
 Floyd-Warshall dynamic programming update.
However, this assumes that $\state_t$ is being visited for the first time.
If the state $\state_t$ is being visited for the second time, the earlier
value may be the shorter path for it.
\end{enumerate}
\item
\TODO{Drop Q Value} Maintains another Q-value. Why do you need Q-value apart from FW-value?
We need only goal reward and add it to the entire state space.
\end{itemize}

\begin{algorithm}
  Let $\rew_g \leftarrow 10$\;
  Initialize $\fwcost(\state_i, \act_i, \state_j; \param_{\fwcost}) \leftarrow -100$ \;
  Let minimum path reward $\minedgecost \leftarrow -0.05$ \;
  Initialize empty replay memory $\mathcal{R} \leftarrow \{\} $\;
  Initialize empty key frames $\State_K \leftarrow \{\} $\;
  Observe $\meas_0$ \;
  $\state_0 = \ObsEnc(\meas_0; \param_E)$ \;
  \For{$t \leftarrow 1$ \KwTo $\epiT$}{
    Take action $\act_{t-1}$ \;
    Observe $\meas_t$, $\rew_t$ \;
    $\state_t = \ObsEnc(\meas_t; \param_E)$ \;
    \If{$\rew_t > \rew_g$}{
      $\state_g = \state_t$ \;
      break\;
    }
    Record transition $\mathcal{R} \leftarrow
    \{(\state_{t-1}, \act_{t-1}, \rew_t, \state_t)\} \cup \mathcal{R}$ \;
    $\fwcost(\state_{t-1}, \act_{t-1}, \state_t) \leftarrow \rew_t + \minedgecost$ \;
    \For{$\state_k \in \mathcal{R}[\state < \state_t], \act_k \in \Act$ }{
      $\fwcost(\state_k, \act_k, \state_t) \leftarrow
        \max \{
        \fwcost(\state_k, \act_k, \state_t),
        \fwcost(\state_k, \act_k, \state_{t-1})
        + \fwcost(\state_{t-1}, \act_t, \state_t) \}$ \;
      \If{ $\state_t \in \mathcal{R}$ }{
         $\State_K \leftarrow \{ \state_t \} \cup \State_K$ \;
         \For{$\state_g \in \mathcal{R}[\state > \state_t]$ }{
            \tcc{Floyd-Warshall update}
            $\fwcost(\state_k, \act_k, \state_g) \leftarrow
              \max \{
              \fwcost(\state_k, \act_k, \state_g),
              \fwcost(\state_k, \act_k, \state_t) + \max_{\act \in \Action} \fwcost(\state_t, \act, \state_g)
              \}$
              \;
        }
      }
    }
  }
  \KwResult{To follow the shortest path $\state_i$ to $\state_j$, follow the
    neighbors with highest $\qValue$\;
    $\policy(\state_k) = \arg \max_{\act_k \in \Action} \fwcost(\state_k, \act_k, \state_g)$\;
  }
  \caption{\small How to solve small windy grid world with randomized goals?}
  \label{alg:floyd-warshall-small}
\end{algorithm}

\section{Experiments}

\begin{itemize}\item
Grid world: Set up a random goal static maze scenario, compare with normal Q-learning.
\item
\TODO{Lava world}: Set up a Lava world like \cite{schaul2015universal} and test on it.
\item
\TODO{Atari games}: Compare performance with normal Q-learning.
\item
\TODO{Deepmind Lab}: Set up a random goal static maze scenario, compare with normal Q-learning. 
\end{itemize}

\subsection{Grid world with dynamic goals}

We setup a randomly generated grid world and select a randomly generated goal state.


\def\localbib{/home/dhiman/wrk/group-bib/shared}
\IfFileExists{\localbib.bib}{
\bibliography{\localbib,main_filtered,main}}{
\bibliography{main_filtered,main}}
\bibliographystyle{aaai}
\end{document}
