%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{iclr2019_conference}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{cases}
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
\setcounter{secnumdepth}{2}  

\usepackage[ruled,vlined]{algorithm2e}

\input{preamble}

% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
%\title{Do Goal-Conditioned Value Functions need Goal Rewards to Learn?}
%\title{Learning Goal-Conditioned Value Functions without Goal Rewards}
\title{Learning Goal-Conditioned Value Functions with one-step Path rewards
rather than Goal Rewards}

\author{Anonymous}
%\author{Vikas Dhiman$^1$, Shurjo Banerjee$^1$, Jeffrey M. Siskind$^2$ and Jason J.
%Corso$^1$\\
%The University of Michigan$^1$\\
%Purdue University$^2$}

\pdfinfo{
/Title ()
/Author ()}
\begin{document}

\maketitle
\begin{abstract}
    Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal
    state can change for every trial. State-of-the-art algorithms model these
    problems such that the reward formulation depends on the goals,
    to associate them with high reward.
    This dependence introduces additional goal reward resampling steps 
    in algorithms like Hindsight Experience Replay (HER) that reuse
    trials in which the agent fails to reach the goal by recomputing
    rewards as if reached states were psuedo-desired goals. 
    We propose a reformulation of goal-conditioned value functions for
    MGRL that yields a similar algorithm, while
    removing the dependence of reward functions on the goal.
    Our formulation thus obviates the requirement of reward-recomputation that
    is needed by HER and its extensions.
    We also extend a closely related algorithm,
    Floyd-Warshall Reinforcement Learning, from tabular domains to deep
    neural networks for use as a baseline. Our results are competetive with HER while
    substantially improving sampling efficiency in terms of reward
    computation. 
\end{abstract}


\input{intro}

\input{related_work}

\input{background}

\input{method}

\input{experiments}

\input{discussion}


\input{conclusion}

%\input{algorithm}

%\input{future_work}


\def\localbib{/home/dhiman/wrk/group-bib/shared}
\IfFileExists{\localbib.bib}{
\bibliography{\localbib,main,main_filtered}}{
\bibliography{main,main_filtered}}
\bibliographystyle{iclr2019_conference}

%\input{appendix}

%\newpage

%\section{Old Experiments}
%\input{old-experiments}
\end{document}
