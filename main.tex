\def\year{2019}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai19}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{subfig}
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
\setcounter{secnumdepth}{2}  

\usepackage[ruled,vlined]{algorithm2e}

\input{preamble}

% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Floyd-Warshall Reinforcement Learning:\\
Learning from Past Experiences to Reach New Goals}

\author{Anonymous}
%\author{Vikas Dhiman$^1$, Shurjo Banerjee$^1$, Jeffrey M. Siskind$^2$ and Jason J.
%Corso$^1$\\
%The University of Michigan$^1$\\
%Purdue University$^2$}

\pdfinfo{
/Title ()
/Author ()}
\begin{document}

\maketitle
\begin{abstract}
Consider mutli-goal tasks that involve static environments and dynamic
goals. Examples of such tasks, such as goal-directed navigation
and pick-and-place in robotics, abound.
Two types of Reinforcement Learning (RL) algorithms are used for
such tasks: 
\emph{model-free} or \emph{model-based}. Each of these approaches has limitations.
Model-free RL struggles to transfer learned information when the goal location
changes, but achieves high asymptotic accuracy in single goal tasks. Model-based
RL can transfer learned information to new goal locations by retaining the
explicitly learned state-dynamics, but is limited by the fact that small errors in
modelling these dynamics accumulate over long-term planning.
In this work, we improve upon the limitations of model-free RL in
multi-goal domains. 
We do this by adapting the Floyd-Warshall algorithm for RL and call the
adaptation Floyd-Warshall RL (FWRL).
The proposed algorithm learns a goal-conditioned action-value function by 
constraining the value of the
optimal path between any two states to be greater than or equal to the value of
paths via intermediary states.
Experimentally, we show that FWRL is more sample-efficient and learns
higher reward strategies in multi-goal tasks as compared to Q-learning, model-based RL 
and other relevant baselines in a tabular domain.

%The algorithm works by learning a goal-condition action-value function
%that employs constraints inspired by the Floyd-Warshall algorithm
%for path planning.  Specifically, the value of the optimal path
%between any two states is constrained to be greater than or equal to
%the value of path via any intermediate state.


\end{abstract}


\section{ Introduction}

\input{intro}

%\input{claims}

\input{related_work}

\input{background}

\input{method}

\input{experiments}

\input{results}

\input{conclusion}

%\input{algorithm}

\input{future_work}

%\begin{itemize}
%    \item Grid world: Set up a random goal static maze scenario, compare with normal Q-learning.
%    \item Lava world: Set up a Lava world like \cite{schaul2015universal} and test on it.
%    \item \TODO{Deepmind Lab}: Set up a random goal static maze scenario, compare with normal Q-learning. 
%\end{itemize}


\def\localbib{/home/dhiman/wrk/group-bib/shared}
\IfFileExists{\localbib.bib}{
\bibliography{\localbib,main,main_filtered}}{
\bibliography{main,main_filtered}}
\bibliographystyle{aaai}
\end{document}

LaTeX/MPS finished at Mon Sep  3 11:44:55
