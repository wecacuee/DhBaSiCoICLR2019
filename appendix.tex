\section{Appendix}

Our algorithm~\ref{alg:floyd-warshall-deep} is different from
HER~\cite{andrychowicz2016learning} because it contains additional step-loss
term $\LossStep$ at line number 17 which allows the algorithm to learn even when
the rewards received are independent of desired goal. Also in HER sampling (line
13), the algorithm recomputes the rewards because the goal is replaced with a
pseudo-goal. Our algorithm does not need reward recomputation because the reward
formulation does not depend on the goal and is not affected by choice of
pseudo-goal.
Our algorithm is also different from Floyd-Warshall Reinforcement learning
because it does not contain $\LossUp$ and $\LossLo$ terms and contains the
additional $\LossStep$.

\input{algorithm-her-pr}

\section{Ablation on loss and goal rewards}

In Figure~\ref{fig:loss-ablation} and Figure~\ref{fig:path-rewards} we show
ablation on loss functions and goal rewards. In Figure~\ref{fig:path-rewards}
Our method is shown in blue with HER - Goal rewards + $\LossStep$.

\begin{figure*}
\input{figure-fetch-push-loss-vars.tex}\hfill
\input{figure-fetch-pick-and-place-path-rewards.tex}
\end{figure*}