\section{Experiments}

Experiments are conducted in a grid-world like setup as displayed in
figure \ref{fig:four-room-grid-world}. The agent can occupy any one of
the white blank squares. The agent's observations is the numbered
location of each square i.e. each squares x,y coordinate with the origin
being the top left corner of the environment i.e. $s_t = (x, y)$. Agents
can act by moving in the four different cardinal directions, $\{up,
down, left, right\}$. Grid-world is chosen since due to it's simplicity
it easily highlights differences between baseline models and FWRL.
Several different grid-world setups are investigated to test the
abilities of FWRL.


\subsection{Four room grid world}
Four room grid world is a grid world with four rooms connected to each
other as shown in Figure~\ref{fig:four-room-grid-world}. This example is
chosen due to it's intentional difficulty for random exploration based
agents. Since the exit points, are narrow, random agents tend to get
stuck in individual rooms. 

\subsection{Four room windy world}

In four room windy world, the previous setup is augmented with
\emph{wind}. In cells that  wind, shown by arrows, the agent gets pushed
around by the wind with 0.25 probability in the direction of the arrow
irrespective of the action taken. Concieved by \cite{}, the setup
increases the dependence of the dynamics model upon environment
specifics.

\subsection{Random Grid Worlds}
While the prevoius experiments were chosen for the ability to study a
specific property of these algorithms, random grid-world maps are
created and tested upon.

\subsection{Metrics}
The metrics used to quantify and compare agent performance across
both baseline methods and FWRL are described here.

\begin{enumerate}
    \item \textbf{Reward}\\\noindent
        As in typical in reinforcement learning, the reward earned by
        the agent is treated as a metric of success. Since the
        environments used are finite MDPs, Q-Learning is known to learn
        the optimal policy given enough exploration time. Of interest is
        thus the amount of time taken for reward to climb.

    \item \textbf{\Loo}\\
        First defined in \cite{MiPaViICLR2017}, \Loo is the ratio of the
        amount of time taken to hit the goal for the first time to the
        average amount of time taken to hit goals subsequently. It is
        the ratio of the exploration time over the exploitation time. 

    \item \textbf{Distance-Inefficiency}\\
        Introduced in \cite{dhiman2018critical}, the
        distance-inefficiency is the ratio of the distances travelled by
        the agent during an episode to the sum of the shortest path to
        the goal at every point of initialization. 
\end{enumerate}


%
\begin{figure}[h!]%
\includegraphics[width=0.48\columnwidth]{media/4-room-grid-world.pdf}
\hfill
\includegraphics[width=0.48\columnwidth]{media/4-room-windy-world.pdf}%
\caption{Left: Four room grid world. Right: Four room windy grid world with wind direction shown by arrows. The windy pushes the agent in the direction of wind with 0.25 probability irrespective of the action taken.}
\label{fig:four-room-grid-world}%
\end{figure}%

