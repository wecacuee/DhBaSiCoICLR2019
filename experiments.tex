\section{Experiments}
\label{sec:experiments}
We use the environments introduced in \citet{plappert2018multi} for our experiments.
Broadly the environments fall in two categories, Fetch and Hand tasks.
Our results show that learning is possible across all environments
without the requirement of goal-reward.

The Fetch tasks involve a simulation of the Fetch robot's 7-DOF robotic arm. The four tasks are Reach, Push,
Slide and PickAndPlace.
In the Reach task the arm's end-effector is tasked to reach the a particular 3D coordinate. 
In the Push task a block on a table needs to be pushed to a given point on it.
In the Slide task a puck must be slid to a desired location.
In the PickAndPlace task a block on a table must be picked up and moved to a
3D coordinate.

The Hand tasks use a simulation of the Shadow dexterous hand to manipulate objects of
different shapes and sizes. These tasks are HandReach,
HandManipulateBlockRotateXYZ, HandManipulateEggFull and HandManipulatePenRotate.
In HandReach the hand's finger tips need to reach a given configuration.
In the HandManipulateBlockRotateXYZ, the hand needs to rotate a cubic
block to a desired orientation.
For HandManipulateEggFull repeats this orientation tasks with an egg.
To HandManipulatePenRotate does so with a pen.

Snapshots of all these tasks can be found in Figure~\ref{fig:envs}. Note that
these tasks use joint angles, not visual input.


\subsection{Metrics}
Similar to prior work, we evaluate all experiments on two metrics: the success
rate and the average distance to the goal. The success rate is defined as the
fraction of episodes in which the agent is able to reach the goal within a
pre-defined threshold region.
% $\frac{1}{E}\sum_{e,t} \mathbbm{1}_{\|\goal_t - \goal^*\|_2 < \epsilon}$.
The metric \emph{distance of the goal} is the euclidean distance between
the achieved goal and the desired goal.
% $\|\goal_t - \goal^*\|_2$.
These metrics are plotted against the number of training epochs showing
comparable results of our method to the baselines.

To emphasize that our method does not require goal-reward
and reward re-computation, we plot these metrics against
number of reward computations used during training. The number of reward
computations include both the episode rollouts and the reward recomputations
during HER sampling.

%
\input{figure_fetch}

\subsection{Hyper-parameters choices}
Unless specified, all our hyper-parameters are identical to the ones
used in the HER
implementation~\citep{dhariwal2017baselines}. We note two main changes
to HER to make the comparison more fair. Firstly,
we use a smaller \emph{distance-threshold}.
The environment used for HER and FWRL returns the goal-reward when the
achieved goal is within this threshold of the desired goal. Because of
the absence of goal-rewards, the distance-threshold information is not used by our
method.
We reduce the it to 1cm which is reduction by a factor of 5 compared to
HER.

Secondly, due to hardware limitations we run all experiments on 6
cores each
while HER uses 19. The batch size used is a function of the number of
cores and hence this parameter has a significant effect on learning. 

However, to ensure fair comparison, all experiments are run with the same hyper-parameters and
random seeds to ensure that variations in performance are purely due
to differences between the algorithms.

\subsection{Results}
% Due to space limitations and in the interest of clarity we show a subset our experiments across both platforms that emphasize both strengths and weaknesses of our algorithm.
All our experimental results are described below. We highlight the strengths and
weaknesses of our algorithm. Across all our experiments, the
distance-to-the-goal metric achieves comparable performance to HER
\emph{without requiring goal-rewards}. 

\subsubsection{Fetch Tasks}

The experimental results for Fetch tasks are shown in
Figure~\ref{fig:fetch-results}. For Fetch Reach and Push tasks our
method (red)  achieves comparable performance to the baselines 
across both metrics, the success rate and the distance to the goal. In the Fetch
Pick and Place task, our method outperforms the baselines. However, for Fetch
Slide task the opposite is true.
When the number of reward computations is take into account our method learns
much faster than across all tasks.

\subsubsection{Hand Tasks}

For the Hand tasks the distance to the goal and the success rate show different trends.
We show the results in Figure~\ref{fig:hand-results}.

For the distance metric when plotted against epochs, we get comparable
performance for all tasks; when plotted for reward computations we outperform
all baselines.

Similar trends does not hold for the success rate and we find our method to be
consistently under-performing than the baselines across all tasks. We note that
although all algorithms are equally far away from the goal, the success rate of
baselines is better than our method. We conjecture that this might be because of
high distance failure cases of the baselines. The baselines success rate is
high, but when they fail, they fail with a larger distance to the goal. This is
in contrast to our method which fails consistently but with still ends up closer
to the goal. On an average all methods end up showing similar distance from the goal.

\input{figure_hand}
