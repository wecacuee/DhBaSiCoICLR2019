%\section{Experiments}
%\label{sec:experiments}

\section{Experiments}
We use Fetch push, reach and pick and place
tasks~\citep{plappert201802multigoalrl} in our experiments:
%
\begin{description}
  \item[Fetch-Reach] The tip of a robotic arm reaching a desired location.
  \item[Fetch-Push] A robotic arm pushing a block to a desired location.
  \item[Fetch-Slide] A robotic arm sliding a puck to a desired location.
\end{description}%
% 
\subsection{Baseline: Hindsight experience replay}
Hindsight Experience Replay~\cite{andrychowicz2016learning} targets
goal-conditioned tasks.
In goal-conditioned tasks, the rewards can be very sparse. Unless the agent hits
the goal, no value-able reward is learned and rest of the space is almost even
with respect to the rewards. To address this challenge, HER first assumes that
for every $\state_t$ the achieved goal $\goal_t = f_\goal(\state_t)$ is known.
Then HER simulates as if the achieved goal $\goal_t$ was the intended goal all
along by re-sampling the goal conditioned reward function.
More concretely, two time steps $t_1$ and $t_2 > t_1$ from the same episode in the replay memory are
sampled. The achieved goal at $t_2$ , $\goal_{t_2}$ is assumed to be the desired
goal at $t_1$ and the new simulated transition becomes $(\state_{t_1},
\act_{t_1}, \state_{t_1 + 1}, \Rew(\state_{t_1}, \goal_{t_2}, \act_{t_1}))$,
where $\Rew(\state_{t_1}, \goal_{t_2}, \act_{t_1})$ is the recomputed reward
instead of the observed reward $\Rew(\state_{t_1}, \goal^*_{e}, \act_{t_1})$ that
depends on the desired goal $\goal^*_{e}$ for that episode $e$.

HER can be applied to either DDPG or DQN, the experiments in
\citet{andrychowicz2016learning} show them to be applied on DDPG.

\subsection{Goal conditioned tasks: Alternative formulation}
Unlike \citet{andrychowicz2016learning}, we do not believe that reward
information is meaningless without achieving the desired goal state.

We make three assumptions:
\begin{enumerate}
\item We assume that there is no special reward
  on reaching a particular state even the goal state. The reward formulation is
  static even with change in goal states.
  In a graph interpretation, this equivalent to assuming that there are no
  vertex rewards but only edge rewards.
  $\Rew(\state, \goal, \act) = \Rew(\state, \act)$.
\item Like the Floyd-Warshall algorithm we assume no positive reward cycles
  exist i.e. $  \sum_{k=t}^{T-1} \Rew(\state_k, \act_k) \le 0 \text{ if }\state_T = \state_t$.
\end{enumerate}

The benefit of these assumptions is that we do not need to resample the reward
function with simulated goals as HER~\cite{andrychowicz2016learning} need to do.
Resampling the reward function can be expensive.

\subsection{Proposed algorithm: Floyd-Warshall RL}

We present a model-free reinforcement learning method that transfers
learned behavior when goal locations are dynamic. We call this algorithm
Floyd-Warshall Reinforcement Learning, because of its similarity to
Floyd-Warshall algorithm~\cite{floydwarshall1962}:
a shortest-path planning algorithm on graphs. Similar
to universal value function~\cite{schaul2015universal}, we define the Floyd-Warshall
(FW) function as the expected cumulative reward on going from a start
state to an end state:
%
\begin{align}
\fwargs{\state_t = \state}{\act_t = \act}{\goal}{\policy}{} =
\E_{\act_k \sim \policy(\state_t),T}\left[ \sum_{k=t}^{T-1} \Rew(\state_k, \act_k) \middle\vert \state_t = \state, \act_t = \act, \|f_\goal(\state_T) - \goal\|_2 < \epsilon \right] ,
\end{align}%
%

Note that objective is different from reinforcement learning formulated in HER,
as it is conditioned on the goal now. In \citet{andrychowicz2016learning}, the
reward formulation has to reflect that the desired goal to reach. In our formulation
we make it a precondition to the objective function, hence we do not need to
make the reward function dependent upon goal.
We want to find the policy that maximizes the expected reward from any
state $\state_t$ to a desired goal $\goal$:
%
\begin{align}
  \policy^* &=
\arg \max_{\policy} \E_{\state_t, \act_k \sim \policy(\state_t), T}\left[ \sum_{k=t}^{T-1} \Rew(\state_k, \act_k) \middle\vert \state_t = \state, \act_t = \act, \|f_\goal(\state_T) - \goal\|_2 < \epsilon \right],
  \\
  \policy^*(\state, \goal) &= \arg \max_{\act} \fwargs{\state_t = \state}{\act_t = \act}{\goal}{}{*}\\
\text{where } &\fwargs{\state_t = \state}{\act_t = \act}{\goal}{\policy}{*} = 
\max_{\policy} \fwargs{\state_t = \state}{\act_t = \act}{\goal}{\policy}{},
\end{align}%
%

Now all the properties of the Floyd-Warshall function, that we know can be used
as the an objective function.
What are the properties of the Floyd-Warshall
function

\subsubsection{Step loss}
One step reward path is the highest reward path:
\begin{align}
      \Loss_{\text{step}} &= (\fwargs{\state_{b}}{\act_{b}}{\goal_{b+1}}{m}{} - \rew_b)^2
\end{align}
%

\subsubsection{DDPG loss}
This is the one step rollout for a long term goal and stays same if we sample a
long term achieved goal $\goal_{b\text{her}}$ using HER sampling:
\begin{align}
  \Loss_{\text{ddpg}} &= (\fwargs{\state_{b}}{\act_{b}}{\goal_{b\text{her}}}{m}{} -
      \rew_b + \discount\fwargs{\state_{b}}{\policy_t(\state_b, \goal_{b\text{her}};\param_\policy)}{\goal_{b\text{her}}}{t}{})^2
\end{align}
%

\subsubsection{Bounds}
Floyd-Warshall inspires us to come up with these triangular inequality like
bounds that force the highest reward path to be higher that any other path
through an intermediate state.
%
\begin{align}
  \Loss_{\text{upper}} &= |
      \fwargs{\state_{b}}{\act_b}{\goal_s}{t}{}
      + \fwargs{\state_{s}}{\policy_t(\state_s, \goal_{b};\param_\policy)}{\goal_b}{t}{}
      - \fwargs{\state_{b}}{\act_b}{\goal_b}{m}{}
      |_+^2
                         \\
  \Loss_{\text{lower}} &= |
      \fwargs{\state_{b}}{\act_b}{\goal_s}{m}{}
      + \fwargs{\state_{s}}{\policy_t(\state_s, \goal_{b};\param_\policy)}{\goal_b}{t}{}
      - \fwargs{\state_{b}}{\act_b}{\goal_b}{t}{}
      |_+^2
\end{align}%
% 

The resulting algorithm in shown in Alg~\ref{alg:floyd-warshall-deep}. An
ablation of loss functions is shown in Figure~\ref{fig:fig:fwrl-stepfwrl-noop-FetchPush}.

\input{algorithm}


%
\begin{figure}
  \def\frac{0.32}
Loss function breakdown without HER sampling on Fetch Push\\
  \includegraphics[width=\frac\columnwidth]{media/res/f84daa7-FetchPush-v1-stfw-none/test/success_rate.pdf}%
  \includegraphics[width=\frac\columnwidth]{media/res/f84daa7-FetchPush-v1-stfw-none/test/mean_Q.pdf}%
  \includegraphics[width=\frac\columnwidth]{media/res/f84daa7-FetchPush-v1-stfw-none/train/critic_loss.pdf}\\
Loss function breakdown with HER sampling on Fetch Push\\
  \includegraphics[width=\frac\columnwidth]{media/res/3f1eafe-FetchPush-v1-stfw-future/test/success_rate.pdf}%
  \includegraphics[width=\frac\columnwidth]{media/res/3f1eafe-FetchPush-v1-stfw-future/test/mean_Q.pdf}%
  \includegraphics[width=\frac\columnwidth]{media/res/3f1eafe-FetchPush-v1-stfw-future/train/critic_loss.pdf}%
  \caption{
    Fetch Push results. Loss function changes do no seem to make a difference.
    There are four parts to the loss function (1) DDPG Loss $\Loss_{\text{ddpg}}$ ,
    (2) Step loss$\Loss_{\text{step}}$,  
    (3) Lower bound $\Loss_{\text{lower}}$ and
    (4) Upper bound $\Loss_{\text{upper}}$ .
    ddpg = $\Loss_{\text{ddpg}}$,
    dqst = $\Loss_{\text{ddpg}}$ + $\Loss_{\text{step}}$,
    fwrl = $\Loss_{\text{ddpg}}$ + $\Loss_{\text{lower}}$ +
    $\Loss_{\text{upper}}$,
    qlst = $\Loss_{\text{ddpg}}$ + $\Loss_{\text{step}}$ + $\Loss_{\text{lower}}$ + $\Loss_{\text{upper}}$.
    stfw = $\Loss_{\text{step}}$ + $\Loss_{\text{lower}}$ + $\Loss_{\text{upper}}$,
    stlo = $\Loss_{\text{step}}$ + $\Loss_{\text{lower}}$,
    stup = $\Loss_{\text{step}}$ + $\Loss_{\text{upper}}$.
    Success rate is the fraction of times the agent reaches the goal. Q(test) is
    the estimated cumulative reward by the network. Critic loss is the total
    loss plotted during training.
    Because stfw, stlo, stup fail to succeed, we infer that the $\Loss_{\text{ddpg}}$ DDPG loss is
    critical for making the algorithm work. Since the qlst works better than
    fwrl, we infer that $\Loss_{\text{step}}$ Step loss is also important.
    only.
    Since there is slight improvement in dqst over ddpg, this means
    $\Loss_{\text{step}}$ really helps. dqst did not run fully but it shows
    promise (I need to fix a bug).
    But why does the loss for stfw keep rising? Does it mean that the SGD is not
    able to optimize the loss gradients in the right direction?
  }%
  \label{fig:fwrl-stepfwrl-noop-FetchPush}%
\end{figure}%
% 


\subsection{Unanswered questions and things to try}

\subsubsection{FWRL specific sampling}
Right now the shuffle step in the algorithm is totally random and probably
introduces more noise in the algorithm than it helps. A modification of HER
sampling would sampling three time steps from the trajectory (single episode)
$t_1 > t_2 > t_3$ and use $t_2$ as the intermediate state for
$\Loss_{\text{upper}}$ and $\Loss_{\text{lower}}$.


\subsubsection{Why is any loss term with upper/lower worse?}
This is probably answered by  the above section but what are the other
explanations. The total ``Critic loss'' is increasing for stfw
($=\Loss_{\text{step}}$ + $\Loss_{\text{lower}}$ + $\Loss_{\text{upper}}$),
which seems to say that with $\Loss_{\text{ddpg}}$, it is hard to optimize the functions.


\subsection{Is it still a contribution if the upper and lower bounds do not
  improve the results?}
Can we claim that this alternative formulation is new and more principled than HER?
