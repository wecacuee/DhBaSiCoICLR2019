\section{Introduction}

% Reinforcement learning and it's uses
% Why we care about Multi-Goal Reinforcement Learning
From playing Atari games from raw pixel values \citep{mnih2013playing},
beating world GO champions \citep{silver2017mastering} and applications
to robotic navigation \citep{zhu2017target} and manipulation
\citep{gu2017deep}, the accomplishments of Deep Reinforcement Learning
(DRL) have been wide and varied. The advent of such powerful DRL
algoritms for continous control \cite{lillicrap2015continuous} has seen
an increase in the application of these algorithms to complicated multi-goal tasks
\citep{andrychowicz2017hindsight,pong2018temporal}. In
Multi-Goal Reinforcement Learning (MGRL)\citep{plappert2018multi},
agents in environments are tasked with achieving multiple goal
conditions in as little time as possible.  Such tasks permeate the field
of \emph{robotics}.  For example, in \emph{manipulation} a robotic arm
may need to move an object to a certain location on a table. In
\emph{navigation}, a robot may be tasked with finding a goal locations
in both old and new environments.  


% A common way to solve MGRL problems is the use of Goal-Conditioned
% Value functions. Solving several tasks instead of individual tasks
One of the most popular methods for solving MGRL problems is the use of
\emph{goal-conditioned value functions} (GCVFs), a more generalizable
version of \emph{value functions} in reinforcement learning. Operating
over both state and goal space, a GCVF, $V(s,g)$, represents the
expected cumulative return of an agent reaching any goal state from any
start state. GCVFs are driven by the intuition of solving several tasks
in an environment simultaneously is more generalizable and powerful than
solving single tasks as is normal in most reinforcement learning
problems. 

% Goal conditioned value functions make a big assumption however. It is
% assumed that to learn to achieve goals, goal locations must themselves
% must be associated with reward. This however is not the case. We
% introduce .... 
Much work have showcased the utilization of GCVFs on a variety of
experimental setups \citep{sutton2011horde,
schaul2015universal,andrychowicz2017hindsight}.  Inherited from standard
RL formulations, these functions are usually learned via policy-gradient
or Q-Learning methodologies i.e. algorithms that allow for the learning
of standard value functions. Perhaps central to the learning of these
functions is the idea that on achieving the goal state, agent's are
provided with \emph{goal-rewards}, reward values that are traditionally
larger than reward available in the rest of the state space.  



We introduce \emph{step-loss} that allows for \emph{goal-independent
reward formulations}. We show that this reward formulation leads to
increased sample efficiency for a variety of    

% A secondary contribution of this work is the extension of the FWRL
% algorithm to the Deep RL domain. We leverage compositionality
% constraints in the space of FWRL to introduce a new algorithm that is
% more principled. 


% In summary, our contributions are and results show increased sample
% efficiency. 
In summary, the  contributions of this work are threefold. We (1)
Describe and introduce \emph{step-loss}, a simple augmentation that
allows for \emph{goal-independent reward formulations}. (2) We showcase
how goal-conditioned value functions can be learned under such
\emph{goal-independent reward formulations} leading to substantially
improved sample efficiency across a variety of algorithms and (3) we
extend Floyd-Warshal Reinforcement Learning for Tabular Domains to the
realm of DRL leveraging compositionality constraints in the space of
goal-conditioned value functions. 


%% There is inherent structure in the formulation of these functions
%% thathas been ignored. Dhiman et al introduced structure in the space
%% of these functions 
%Floyd-Warshall Reinforcement Learning was first introduced in Dhiman et.
%al. for tabular domains. 
%The FW functions is defined 
%%
%\begin{align}
%\fwargs{\state}{\act}{\state'}{\policy}{} =
%\E_{\policy}\left[ \sum_{t=0}^{t=k} \rew_t \middle\vert \state_0 = \state, \act_0 = \state, \state_k = \state' \right] .
%\end{align}%
%%
%which is the expected sum of rewards when the state is s and the
%end state is the goal. Compositionality constraints are then used to
%learn the FW function.
%%
%\begin{align}
%\fwargs{\state_i}{\act_i}{\state_j}{\policy^*_{\state_j}}{*}
% \ge 
%  \fwargs{\state_i}{\act_i}{\state_k}{\policy^*_{\state_k}}{*}
%  + \max_{\act_k}\fwargs{\state_k}{\act_k}{\state_j}{\policy^*_{\state_j}}{*}
%  ,\forall \state_k \in \State.
%  \label{eq:fwconstraint}
%\end{align}%
%%
%The constraint is inspired by the Floyd-Warshall algorithm for path
%planning on graphs and states that the expected reward in traversing
%from one state to another should be greater than or equal to that of the
%paths through intermediary states.  While Dhiman et al. was restritcted
%to toy examples in the gridworld doman and tabular functions, they are
%not naturally scalabale to real life problems. This work extends their
%formulation to paramteric functions like neural networks by addition
%terms to the RL objective based on these compositionality constraints.
%Since we extend the algorithm to the Deep Learning domain, we call it
%Deep Floyd-Warshall Reinforcement Learning. 






