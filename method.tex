\section{Method}

\subsubsection{Goal conditioned value function: Path Reward Formulation}
We define the goal-condition value function as expected path-rewards of reaching
the goal from a given start state:
%
\begin{align}
  \fwargs\state\act{\goal^*}\policy{P}
  &=
    \begin{cases}
      \E_{l \in [t+1, T], \policy}\left[ \sum_{k=t}^{l-1}
  \discount^{k-t} \Rew^P(\state_k, \act_k)
  \middle| \state, \act, \goal_l = \goal^* \right]
& \text{ if } \exists l \text{ such that } \goal_l = \goal^*
\\
-\infty & \text{ otherwise }
\end{cases}.
  \label{eq:qp-def}
\end{align}%
% 
The main difference between $\fw^P$~\eqref{eq:qp-def} and $\fw$~\eqref{eq:q-def}
is that $\fw^P$ explicitly depends upon reaching the goal within the episode,
thereby removing the dependence of reward on the goal. Surprisingly, the Bellman
Equation can still be applied to the path-value function $\fw^P$ with a slightly
different terminal condition:
%
\begin{align}
  \fwargs{\state_t}{\act_t}{\goal^*}*P
  &=
    \begin{cases}
      \Rew^P(\state_t, \act_t) + \discount \max_{\act \in \Action} \fwargs{\state_{t+1}}\act{\goal^*}*P
      & \text{ if } t < l-1 \text{ such that } \goal_l = \goal^*
      \\
      \Rew^P(\state_{l-1}, \act_{l-1}) & \text{ if } t = l-1 \text{ such that } \goal_l = \goal^*
    \end{cases}.
  \label{eq:bellman-path}
\end{align}%
% 
This formulation has several advantages. Firstly, it removes a design decision
about the choice of goal-reward. Secondly, it enables HER to work without reward
re-computation. This can be especially useful when reward computation is expensive.

\subsubsection{One-Step Loss}
Because we work with path rewards, the reward function design should
make sure that there are no positive cycles in the environment. In another
words, cycling back to the same state via another state should not yield
positive rewards, otherwise the agent can extract unlimited reward just by
going in circles. Hence a typical environment will have most of the reward
values as negative. In all our experiments with path rewards, we use a constant
value of -1 for all states, $\Rew^P(\state, \act) = -1 \, \forall \state, \act$.
Since our new formulation of goal-conditioned value function satisifies the
Bellman Equation, the existing MGRL algorithms for $\fw$ easily apply to the
approximation of $\fw^P$ as long as we can estimate the terminal reward for
Eq~\eqref{eq:qp-def}. The terminal reward, however, depends up on detecting
whether the goal has been reached. To avoid this dependence, we introduce the
step loss $\LossStep$ which assumes that one-step reward is the highest reward between
adjacent start-goal state:
%
\begin{align}
      \LossStep(\param_\fw) &= (\fwargs{\state_{l-1}}{\act_{l-1}}{\goal_{l};\param_\fw}{*}{P} - \Rew(\state_{l-1}, \act_{l-1}))^2.
\end{align}
%
This loss term allows us to estimate one-step path reward for all start and goal
states.
We modify an implementation of HER to include the step-loss term and disable goal
rewards for our experiments.
As in HER, we use the loss term from DDPG~\cite{lillicrap2015continuous} while
using ``future'' goal sampling strategy described in the paper.
For this purpose an episode-wise replay buffer of transitions $(\state_t,
\goal_t, \act_t, \rew_t, \state_{t+1}, \goal_{t+1}; \goal^*)$ is maintained. To draw samples
from the replay buffer an episode and time step is chosen. The corresponding
transition is taken to the sampled transition, however, the desired goal for
this transition $\goal^*$ is replaced by randomly chosen goal from the future
transitions $\goal_{t+f}$. This new transition is applied to minimize the
following DDPG loss function:
%
\begin{align}
  \LossDDPG(\param_\fw) &= (\fwargs{\state_{t}}{\act_{t}}{\goal_{t+f}; \param_\fw}{*}{P} -
      \rew_t - \discount\fwargs{\state_{t+1}}{\policy_t(\state_{t+1}, \goal_{t+f};\param_\policy)}{\goal_{t+f};\param_\fw}{t}{P})^2
\end{align}
%

\subsection{Deep Floyd-Warshall Reinforcement Learning}

We extend FWRL~\citep{dhiman2018floydwarshall} to use deep neural network.
The main contribution of FWRL is to employ compositionality constraints to
exploit structure in the space of GCVFs.
We translate these constraints into loss terms.
In effect, the algorithm is same as the HER except
addition of a few additional loss terms.


The constraint governing the FWRL algorithm states that the highest reward path
from any state $\state_t$ to any goal $\goal_{t+f}$ is greater than or equal to
the sum of rewards via any intermediate state-goal pair $(\state_{im}, \goal_{im})$:
%
\begin{align}
      \fwargs{\state_{t}}{\act_t}{\goal_{im}}{*}{}
      + \fwargs{\state_{im}}{\policy_*(\state_{im}, \goal_{t+f};\param_\policy)}{\goal_{t+f}}{*}{}
      \ge \fwargs{\state_{t}}{\act_t}{\goal_{t+f}}{*}{},
\end{align}%
% 
where $\policy_*$ is the optimal goal-conditioned policy and $\act_t$ represents
the action taken at time $t$. Taking cue from \citet{MnKaSiNATURE2015}, we do
not repeat the main online network $\Q_*$ in the loss term. We use target
network $\Q_t$ and split the constraint into two loss terms. One loss term acts
as a lower bound $\LossLo$ and the other acts as the upper bound $\LossUp$:
%
\begin{align}
  \LossLo &= (
      \fwargs{\state_{t}}{\act_t}{\goal_{im}}{t}{}
      + \fwargs{\state_{im}}{\policy_t(\state_{im}, \goal_{t+f};\param_\policy)}{\goal_{t+f}}{t}{}
      - \fwargs{\state_{t}}{\act_t}{\goal_{t+f}}{*}{}
      )_+^2
                         \\
  \LossUp &= (
      \fwargs{\state_{t}}{\act_t}{\goal_{im}}{*}{}
      + \fwargs{\state_{im}}{\policy_t(\state_{im}, \goal_{t+f};\param_\policy)}{\goal_{t+f}}{t}{}
      - \fwargs{\state_{t}}{\act_t}{\goal_{t+f}}{t}{}
      )_+^2,
\end{align}%
% 
where $(\dots)_+$ denotes ReLU function.
Note that the above terms differ only by choice of target network $\fw_t$ and
main network $\fw_*$. 

Similar to HER we get the intermediate state by trajectory sampling. Once,
a transition $(\state_t, \act_t, \rew_t, \state_{t+1})$ and a future goal
$\goal_{t+f}$ has been sampled from the same episode, we sample another
intermediate state and goal pair $(\state_{im}, \goal_{im})$ such that $t \le im
\le t + f$.
% 
% \begin{align}
%   \LossTrieq &= |
%       \fwargs{\state_{b}}{\act_b}{\goal_s}{t}{}
%       + \fwargs{\state_{s}}{\policy_t(\state_s, \goal_{b};\param_\policy)}{\goal_b}{t}{}
%       - \fwargs{\state_{b}}{\act_b}{\goal_b}{m}{}
%       |^2
% \end{align}

The resulting algorithm in shown in Alg~\ref{alg:floyd-warshall-deep}.
% An
%ablation of loss functions is shown in Figure~\ref{fig:fig:fwrl-stepfwrl-noop-FetchPush}.

\input{algorithm-her-pr}
