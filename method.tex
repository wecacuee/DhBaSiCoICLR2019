\section{Path Reward-Based GCVFs}

%\section{Goal conditioned value function: Path Reward Formulation}
In our definition of the GCVF, instead of making the reward function depend upon
the goal, we count accumulated rewards over a path, \emph{path-rewards},  only if the goal is reached. This makes
the dependence on the goal explicit instead  of implicit to the reward formulation.
Mathematically,
%
\begin{subnumcases}{
    \fwargs\state\act{\goal^*}\policy{P} = }
  \E_{\policy}\left[ \sum_{k=t}^{l-1} \discount^{k-t} \Rew^P(\state_k, \act_k)
    \middle| \state, \act, \goal_l = \goal^* \right]
  & $\text{ if } \exists \,l \text{ such that } \goal_l = \goal^*$
  \label{eq:qp-def-a}
  \\
  -\infty & \text{otherwise},
  \label{eq:qp-def-b}
\end{subnumcases}
% 
where $l$ is the time step when the agent reaches the goal. If the agent does
not reach the goal, the GCVF is defined to be negative infinity. This first term
\eqref{eq:qp-def-a} is the expected cumulative reward over
paths from a given start state to the goal.
This imposes the constraint that
cyclical paths in the state space must have negative cumulative reward for 
\eqref{eq:qp-def-a} to yield finite values. For most practical physical
problems, this constraints naturally holds if reward is taken to be some measure of
negative energy expenditure. For example, in the robot arm experiment, moving the
arm must expend energy (negative reward). Achieving a positive reward cycle
would translate to generating infinite energy .
In all our experiments with this formulation, we use a constant
reward of -1 for all states, $\Rew^P(\state, \act) = -1 \, \forall \state, \act$.

For the cases when the agent does reach the goal at time step $l$ \eqref{eq:qp-def-a},
the Bellman equation takes the following form:
%
\begin{subnumcases}{
    \label{eq:bellman-path}
  \fwargs{\state_t}{\act_t}{\goal^*}*P =}
      \Rew^P(\state_t, \act_t) + \discount \max_{\act \in \Action}
      \fwargs{\state_{t+1}}\act{\goal^*}*P 
      & $\text{ if } t < l-1$
      \label{eq:bellman-path-a}
      \\
      \Rew^P(\state_{l-1}, \act_{l-1})
      & $\text{ if } t = l-1$
      \label{eq:bellman-path-b}.
\end{subnumcases}
% 
Notice that terminal step in this equation is the step to reach the goal.
This differs from Equation~\eqref{eq:bellman-dqn}, where the terminal step
is the step at which the episode ends.
%This equation differs from Eq~\eqref{eq:bellman-dqn} in that the
%terminal step is now the step to reach the goal instead of being
%the time step at which the episode ends.
This formulation is equivalent to the end of episode occuring
immediately when the goal is reached.
This reformulation does not require goal-rewards, which in turn obviates the
requirement for pseudo-goals and reward recomputation.

\paragraph{One-Step Loss}
To enable algorithms like HER to work under this reformulation we need to
recognize when the goal is reached~\eqref{eq:bellman-path-b}. This recognition
is usually done by the reception of high goal reward. Instead, we
use ~\eqref{eq:bellman-path-b} as a \emph{one-step loss} that serves
this purpose which is one of our main contributions:
%
\begin{align}
      \LossStep(\param_\fw) &= (\fwargs{\state_{l-1}}{\act_{l-1}}{\goal_{l};\param_\fw}{*}{P} - \Rew(\state_{l-1}, \act_{l-1}))^2.
\end{align}%
%
This loss is based on the assumption that one-step reward is the highest reward
between adjacent start-goal states and allows us to estimate the one-step
reward between them. Once learned, it serves as
a proxy for the reward to the last step to the goal~\eqref{eq:bellman-path-b}.
The Bellman equation~\eqref{eq:bellman-path}, serves as a one-step rollout
to combine rewards to find maximum reward paths to the goal.

One-step loss is different from the terminal step
of Q-Learning because one-step loss is applicable to every transition unlike the
terminal step. However, one-step loss can be thought of as Q-Learning where
every transition is a one-step episode where the reached goal is the pseudo
goal.

One-step loss also different from the terminal condition in
\citet{kaelbling1993learning}. \citet{kaelbling1993learning} defines
$\fwargs{.}{.}{.}{*}{P}$ similar to Eq~\eqref{eq:bellman-path}
except the terminal condition is defined as $\fwargs{\state_t}{\act}{\goal*}{*}{P}
= 0$ when $\goal_t = \goal^*$. Under the stated assumptions, the two definitions
are equivalent but one-step loss is slightly advantageous.
One-step loss can be applied to every transition unlike
the \cite{kaelbling1993learning} terminal condition which can be only applied
when $\goal_t = \goal$.


We modify an implementation of HER to include the step-loss term and disable goal
rewards for our experiments.
As in HER, we use the DDPG loss $\LossDDPG$ while
using the ``future'' goal sampling strategy described in the paper.
The details of the resulting algorithm are shown as pseudo-code in
Algorithm~\ref{alg:floyd-warshall-deep} in the Appendix.

\subsection{Deep Floyd-Warshall Reinforcement Learning}

The GCVF redefinition and one step-loss introduced in this paper are inspired by
the tabular formulation of Floyd-Warshall
Reinforcement Learning (FWRL)~\citep{kaelbling1993learning}.
We extend this algorithm for use with deep neural networks.
Unfortunately, the algorithm itself does not show significant improvement over
the baselines. However, the intuitions gained in its implementation led to
the contributions of this paper. 

The core contribution of FWRL is a compositionality constraint in the space
of GCVFs. 
This constraint states that the optimal $\Q_*$ value 
from any state $\state_t$ to any goal $\goal_{t+f}$ is greater than or equal to
the sum of optimal $\Q_*$ values via any intermediate state-goal pair $(\state_{w}, \goal_{w})$:
%
\begin{align}
      \fwargs{\state_{t}}{\act_t}{\goal_{w}}{*}{}
      + \fwargs{\state_{w}}{\policy_*(\state_{w}, \goal_{t+f};\param_\policy)}{\goal_{t+f}}{*}{}
      \ge \fwargs{\state_{t}}{\act_t}{\goal_{t+f}}{*}{}.
\end{align}%
% 

We translate these constraints into loss terms and add them to the
DDPG loss $\LossDDPG$ and one-step loss $\LossStep$.
Taking cue from \citet{mnih2015human}, we do
not repeat the the main online network $\Q_{m}$ in the loss term. We use
a target network $\Qtgt$ and split the constraint into two loss terms. One loss term acts
as a lower bound $\LossLo$ and the other acts as an upper bound $\LossUp$:
%
\begin{align}
  \LossLo &= \text{ReLU}[
      \fwargs{\state_{t}}{\act_t}{\goal_{w}}{\tgt}{}
      + \fwargs{\state_{w}}{\policy_t(\state_{w}, \goal_{t+f};\param_\policy)}{\goal_{t+f}}{\tgt}{}
      - \fwargs{\state_{t}}{\act_t}{\goal_{t+f}}{m}{}
      ]^2
                         \\
  \LossUp &= \text{ReLU}[
      \fwargs{\state_{t}}{\act_t}{\goal_{w}}{m}{}
      + \fwargs{\state_{w}}{\policy_t(\state_{w}, \goal_{t+f};\param_\policy)}{\goal_{t+f}}{\tgt}{}
      - \fwargs{\state_{t}}{\act_t}{\goal_{t+f}}{\tgt}{}
      ]^2.
\end{align}%
% 
Note that the above terms differ only by choice of the target and main network. 

\paragraph{FWRL Sampling}
We augment HER sampling to additionally get the intermediate state-goal pair
$(\state_{w}, \goal_{w})$.
Once a transition $(\state_t, \act_t, \rew_t, \state_{t+1})$ and a
future goal $\goal_{t+f}$ have been sampled from the same episode, we sample
another intermediate state and goal pair $(\state_{w}, \goal_{w})$ such that
$t \le w \le t + f$.
% 
% \begin{align}
%   \LossTrieq &= |
%       \fwargs{\state_{b}}{\act_b}{\goal_s}{t}{}
%       + \fwargs{\state_{s}}{\policy_t(\state_s, \goal_{b};\param_\policy)}{\goal_b}{t}{}
%       - \fwargs{\state_{b}}{\act_b}{\goal_b}{m}{}
%       |^2
% \end{align}

% An
%ablation of loss functions is shown in Figure~\ref{fig:fig:fwrl-stepfwrl-noop-FetchPush}.

