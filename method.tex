\section{Problem definition}

%\subsection{Environment Setup}
Consider an agent interacting with an environment, $\varepsilon$. At
every time step, $t$, the agent takes an action $\act_t \in \Act$, observes a
state, $\state_t \in \State$ and a reward $\rew_t \in [-\Rgoal, \Rgoal]$.
A goal state, $\goal \in \State$, is provided to the agent and it receives
$\Rgoal$--the highest reward in the environment--on reaching it
with respect to some threshold $\|\state -\goal\| < \delta_{\text{goal-thresh}}$
An episode is defined as of a fixed number of time steps, $T$. For
every episode, a new goal state is provided to the agent as input. If the agent
reaches the goal state before the episode ends, the agent is 
re-spawned at a new random location within the environment while the goal state
remains unchanged for the episode.
The agent's objective is to find the sequence of actions to take that maximizes
the total reward per episode. 

%Once the agent reaches the goal $\|\state_t - \goal\| <
%\delta$

%
%\begin{align}
%\policy^*(\state_t ; \goal) = \act^*_t = \arg \max_{\act_t} \E_{\policy}\left[ \sum_{t=0}^T \rew_t \right]
%\end{align}%
%

%\subsection{Why is this problem important?}
%Many real world problems can be formulated in this context. Consider a robot
%in a new environment.
%The salesman has to explore the city and find the buildings that match the given
%address. The next time the postman gets the same address, they can use their
%experience to find out the building. Even when a new address is provided in the
%next episode, the postman can use experience to find the new address in shorter
%time.
%
%In robotics, tasks like picking and placing the object at a desired
%location can be formulated as goal-directed navigation.

%\subsection{Why is the problem hard?}
Model-free Reinforcement learning methods assume that the rewards are
being sampled from the a static reward function.  In a problem where the
goal location changes it becomes hard to transfer the learned
value-function or action-value function to the changed location.  One
alternative is to concatenate the goal location the state, making the
new state space $[\state_t, \goal]^\top \in \State^2$ larger.  This
method is wasteful in computation and more importantly in sample
complexity \cite{schaul2015universal}.

\section{Method}
\label{sec:method}
We present a model-free reinforcement learning method that transfers
learned behavior when goal locations are dynamic. We call this algorithm
Floyd-Warshall Reinforcement Learning, because of its similarity to
Floyd-Warshall algorithm~\cite{floydwarshall1962}:
a shortest-path planning algorithm on graphs. Similar
to universal value function~\cite{schaul2015universal}, we define the Floyd-Warshall
(FW) function as the expected cumulative reward on going from a start
state to an
end state within an episode:
%
\begin{align}
\fwargs{\state}{\act}{\state'}{\policy}{} =
\E_{\policy}\left[ \sum_{t=0}^{t=k} \rew_t \middle\vert \state_0 = \state, \act_0 = \state, \state_k = \state' \right] ,
\end{align}%
%
where $\policy$ is the
stochastic policy being followed.
Note that we do not use discounted rewards, instead assuming that episodes are
of finite time length. In keeping with the Floyd-Warshall shortest path
algorithm, we assume that there are no positive reward
cycles in the environment.

Note that the FW-function is closely related to the Q-function,
\begin{align}
  \Q_\policy(\state, \act) = \sum_{\state'} P_\policy(\state' | \state, \act) \fwargs{\state}{\act}{\state'}{\policy}{},
\end{align}%
%
where $P_\policy(\state' | \state, \act)$ is the probability of the agent
arriving at $\state'$ within the episode. We define the optimal FW-function as
the maximum expected value that a path between a start state and a goal state can
yield,
\begin{align}
\fwargs{\state}{\act}{\state'}{\policy^*_{\state'}}{*} =
\max_{\policy_{\state'}}  \fwargs{\state}{\act}{\state'}{\policy_{\state'}}{}
\end{align}%
where $\policy^*_{\state'}$ is the
optimal policy towards the goal state $\state'$. Once the
FW-function approximation is learned, the optimal policy can be computed from
FW-function similar to the Q-learning algorithm, $\policy^*_{\state'}(\state) =
\arg \max_{\act} \fwargs{\state}{\act}{\state'}{}{*}$

When the policy is optimal, the Floyd-Warshall function must satisfy the
constraint
%
\begin{multline}
\fwargs{\state_i}{\act_i}{\state_j}{\policy^*_{\state_j}}{*}
 \ge 
  \fwargs{\state_i}{\act_i}{\state_k}{\policy^*_{\state_k}}{*}
  + \max_{\act_k}\fwargs{\state_k}{\act_k}{\state_j}{\policy^*_{\state_j}}{*}
  \\
  \forall \state_k \in \State.
\end{multline}%
%
In other words, the value for the optimal path from given start state to a given
goal state should be greater than or equal to value of path via any intermediate
state.
This triangular-inequality like constraint is the main contribution of our work.
To the best of our knowledge it has not been employed in any previous works
utilizing goal-conditioned value functions.

Aside from the dynamic goal locations, we assume environments underlying reward
distributions to be static. We also assume that the goal reward is the
highest reward in the reward space.
The pseudo-code for the algorithm in shown Alg~\ref{alg:floyd-warshall-small}.


\input{algorithm}

