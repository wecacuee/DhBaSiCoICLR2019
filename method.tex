\section{Method}
Since our new formulation of goal-conditioned value function satisifies the
Bellman Equation, the existing MGRL algorithms for $\fw$ easily apply to the
approximation of $\fw^P$ as long as we can estimate the terminal reward for
Eq~\eqref{eq:qp-def}. The terminal reward, however, depends up on detecting
whether the goal has been reached. To avoid this dependence, we introduce the
step loss $\LossStep$ which assumes that one-step reward is the highest reward between
adjacent start-goal state:
%
\begin{align}
      \LossStep(\param_\fw) &= (\fwargs{\state_{l-1}}{\act_{l-1}}{\goal_{l};\param_\fw}{*}{P} - \Rew(\state_{l-1}, \act_{l-1}))^2.
\end{align}
%
This loss term allows us to estimate one-step path reward for all start and goal
states.
We modify an implementation of HER to include the step-loss term and disable goal
rewards for our experiments.
As in HER, we use the loss term from DDPG~\cite{lillicrap2015continuous} while
using ``future'' goal sampling strategy described in the paper.
For this purpose an episode-wise replay buffer of transitions $(\state_t,
\goal_t, \act_t, \rew_t, \state_{t+1}, \goal_{t+1}; \goal^*)$ is maintained. To draw samples
from the replay buffer an episode and time step is chosen. The corresponding
transition is taken to the sampled transition, however, the desired goal for
this transition $\goal^*$ is replaced by randomly chosen goal from the future
transitions $\goal_{t+f}$. This new transition is applied to minimize the
following DDPG loss function:
%
\begin{align}
  \LossDDPG(\param_\fw) &= (\fwargs{\state_{t}}{\act_{t}}{\goal_{t+f}; \param_\fw}{*}{P} -
      \rew_t - \discount\fwargs{\state_{t+1}}{\policy_t(\state_{t+1}, \goal_{t+f};\param_\policy)}{\goal_{t+f};\param_\fw}{t}{P})^2
\end{align}
%

\subsection{Proposed algorithm: Floyd-Warshall RL}

We present a model-free reinforcement learning method that transfers
learned behavior when goal locations are dynamic. We call this algorithm
Floyd-Warshall Reinforcement Learning, because of its similarity to
Floyd-Warshall algorithm~\cite{floydwarshall1962}:
a shortest-path planning algorithm on graphs. Similar
to universal value function~\cite{schaul2015universal}, we define the Floyd-Warshall
(FW) function as the expected cumulative reward on going from a start
state to an end state:
%
\begin{align}
\fwargs{\state_t = \state}{\act_t = \act}{\goal}{\policy}{} =
\E_{\act_k \sim \policy(\state_t),T}\left[ \sum_{k=t}^{T-1} \Rew(\state_k, \act_k) \middle\vert \state_t = \state, \act_t = \act, \|f_\goal(\state_T) - \goal\|_2 < \epsilon \right] ,
\end{align}%
%

Note that objective is different from reinforcement learning formulated in HER,
as it is conditioned on the goal now. In \citet{andrychowicz2016learning}, the
reward formulation has to reflect that the desired goal to reach. In our formulation
we make it a precondition to the objective function, hence we do not need to
make the reward function dependent upon goal.
We want to find the policy that maximizes the expected reward from any
state $\state_t$ to a desired goal $\goal$:
%
\begin{align}
  \policy^* &=
\arg \max_{\policy} \E_{\state_t, \act_k \sim \policy(\state_t), T}\left[ \sum_{k=t}^{T-1} \Rew(\state_k, \act_k) \middle\vert \state_t = \state, \act_t = \act, \|f_\goal(\state_T) - \goal\|_2 < \epsilon \right],
  \\
  \policy^*(\state, \goal) &= \arg \max_{\act} \fwargs{\state_t = \state}{\act_t = \act}{\goal}{}{*}\\
\text{where } &\fwargs{\state_t = \state}{\act_t = \act}{\goal}{\policy}{*} = 
\max_{\policy} \fwargs{\state_t = \state}{\act_t = \act}{\goal}{\policy}{},
\end{align}%
%

Now all the properties of the Floyd-Warshall function, that we know can be used
as the an objective function.
What are the properties of the Floyd-Warshall
function

\subsubsection{Bounds}
Floyd-Warshall inspires us to come up with these triangular inequality like
bounds that force the highest reward path to be higher that any other path
through an intermediate state.
%
\begin{align}
  \LossUp &= |
      \fwargs{\state_{b}}{\act_b}{\goal_s}{t}{}
      + \fwargs{\state_{s}}{\policy_t(\state_s, \goal_{b};\param_\policy)}{\goal_b}{t}{}
      - \fwargs{\state_{b}}{\act_b}{\goal_b}{m}{}
      |_+^2
                         \\
  \LossLo &= |
      \fwargs{\state_{b}}{\act_b}{\goal_s}{m}{}
      + \fwargs{\state_{s}}{\policy_t(\state_s, \goal_{b};\param_\policy)}{\goal_b}{t}{}
      - \fwargs{\state_{b}}{\act_b}{\goal_b}{t}{}
      |_+^2
\end{align}%
% 

\begin{align}
  \LossTrieq &= |
      \fwargs{\state_{b}}{\act_b}{\goal_s}{t}{}
      + \fwargs{\state_{s}}{\policy_t(\state_s, \goal_{b};\param_\policy)}{\goal_b}{t}{}
      - \fwargs{\state_{b}}{\act_b}{\goal_b}{m}{}
      |^2
\end{align}

The resulting algorithm in shown in Alg~\ref{alg:floyd-warshall-deep}. An
ablation of loss functions is shown in Figure~\ref{fig:fig:fwrl-stepfwrl-noop-FetchPush}.

\input{algorithm}
