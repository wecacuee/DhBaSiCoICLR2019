\section{Problem definition}

\subsection{Environment Setup}
\newcommand{\Rgoal}{R_{\text{goal}}}
Consider an agent interacting with an environment, $\varepsilon$. At
every time step, $t$, the agent observes a state, $\state_t \in \State$,
where $\State$ is the observation state space. The agent can traverse
the state space by taking actions, $\act \in \Act$, where $\Act$ is a
fixed action space. A goal state, $\goal \in \State$, is specified to
the agent where the goal state is a specific observation in the state
space.  $\Rgoal$ is the reward recieved by the agent for finding the
goal state and constitutes the largest reward in the environment.  For
every time step $t$, the agent takes an action $\act_t$, observes a
state $\state_t \in \State$ and receives a reward $\rew_t \in [-\Rgoal,
\Rgoal]$.  Episodes are of a fixed number of time steps, $T$. For every
episode, a randomized goal state is provided to the agent as input. If
the goal state is observed by the agent during the course of an episode,
the agent is randomly reinitialized within the environment while the
goal state remains unchanged.

As is typical in RL domains, the agent's objective is to find the
sequence of actions to take that maximizes the total reward from episode
to episode. Since the environment itself is static, this is best
achieved via the agent first discovering the goal location and then
traversing the shortest path to it from every subsequent \emph{spawn}
state during the course of an episode. The agent is best suited by
algorithms that emphasize the  transfer \emph{environment structure}
from episode to episode. 


%Once the agent reaches the goal $\|\state_t - \goal\| <
%\delta$

%
%\begin{align}
%\policy^*(\state_t ; \goal) = \act^*_t = \arg \max_{\act_t} \E_{\policy}\left[ \sum_{t=0}^T \rew_t \right]
%\end{align}%
%

\subsection{Why is this problem important?}
Many real world problems can be formulated in this context.

Consider a traveling postman problem who has moved into a new city. The
postman has to explore the city and find the buildings that match the
given address.  The next time the postman gets the same address, they
can use their experience to find out the building.  Even when a new
address is provided (in the next episode), the postman can use
experience to find the new episode more quickly.

In robotics, tasks like picking and placing the object at a desired
location can be formulated as goal-directed navigation.

\subsection{Why is the problem hard?}
Model-free Reinforcement learning methods assume that the rewards are
being sampled from the a static reward function.  In a problem where the
goal location changes, hence the reward function also changes, it
becomes hard to transfer the learned value-function or action-value
function to the changed location.  One alternative is to concatenate the
goal location the state, making the new state space $[\state_t,
\goal]^\top \in \State^2$ larger.  This method is wasteful in
computation and more importantly in sample complexity.

\section{Method}
We present a model-free reinforcement learning method that easily
transfers when goal location is dynamic.  We accomplish this by
maintaining a path based expected reward function from any state to any
goal state.  We call this algorithm Floyd-Warshall Reinforcement
Learning, because of its similarity to Floyd-Warshall algorithm : a
shortest-path planning algorithm on graphs.  We define Floyd-Warshall
value function as
%
\begin{align}
\fwcost_{\policy}(i, l,  j) =
\E_{\policy}\left[ \sum_{t=0}^{t=k} \rew_t \middle\vert \state_0 = i, \act_0 = l, \state_k = j \right] .
\end{align}%
%
When the policy is optimal, the Floyd-Warshall function should satisfy the constraint
%
\begin{align}
\fwcost^*(\state_i, \act_i, \state_j) = \max_{\state_k} \left[
\fwcost^*(\state_i, \act_i, \state_k)
+ \max_{\act_k}\fwcost^*(\state_k, \act_k, \state_j) \right] .
\end{align}%
%

We summarize the algorithm in Alg~\ref{alg:floyd-warshall-small}.

\input{algorithm}

