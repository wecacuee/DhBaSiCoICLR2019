\section{Method}
Since our new formulation of goal-conditioned value function satisifies the
Bellman Equation, the existing MGRL algorithms for $\fw$ easily apply to the
approximation of $\fw^P$ as long as we can estimate the terminal reward for
Eq~\eqref{eq:qp-def}. The terminal reward, however, depends up on detecting
whether the goal has been reached. To avoid this dependence, we introduce the
step loss $\LossStep$ which assumes that one-step reward is the highest reward between
adjacent start-goal state:
%
\begin{align}
      \LossStep(\param_\fw) &= (\fwargs{\state_{l-1}}{\act_{l-1}}{\goal_{l};\param_\fw}{*}{P} - \Rew(\state_{l-1}, \act_{l-1}))^2.
\end{align}
%
This loss term allows us to estimate one-step path reward for all start and goal
states.
We modify an implementation of HER to include the step-loss term and disable goal
rewards for our experiments.
As in HER, we use the loss term from DDPG~\cite{lillicrap2015continuous} while
using ``future'' goal sampling strategy described in the paper.
For this purpose an episode-wise replay buffer of transitions $(\state_t,
\goal_t, \act_t, \rew_t, \state_{t+1}, \goal_{t+1}; \goal^*)$ is maintained. To draw samples
from the replay buffer an episode and time step is chosen. The corresponding
transition is taken to the sampled transition, however, the desired goal for
this transition $\goal^*$ is replaced by randomly chosen goal from the future
transitions $\goal_{t+f}$. This new transition is applied to minimize the
following DDPG loss function:
%
\begin{align}
  \LossDDPG(\param_\fw) &= (\fwargs{\state_{t}}{\act_{t}}{\goal_{t+f}; \param_\fw}{*}{P} -
      \rew_t - \discount\fwargs{\state_{t+1}}{\policy_t(\state_{t+1}, \goal_{t+f};\param_\policy)}{\goal_{t+f};\param_\fw}{t}{P})^2
\end{align}
%

\subsection{Deep Floyd-Warshall Reinforcement Learning}

We extend FWRL~\citep{dhiman2018floydwarshall} to use deep neural network.
The main contribution of FWRL is to employ compositionality constraints to
exploit structure in the space of GCVFs.
We translate these constraints into loss terms.
In effect, the algorithm is same as the HER except
addition of a few additional loss terms.


The constraint governing the FWRL algorithm states that the highest reward path
from any state $\state_t$ to any goal $\goal_{t+f}$ is greater than or equal to
the sum of rewards via any intermediate state-goal pair $(\state_{im}, \goal_{im})$:
%
\begin{align}
      \fwargs{\state_{t}}{\act_t}{\goal_{im}}{*}{}
      + \fwargs{\state_{im}}{\policy_*(\state_{im}, \goal_{t+f};\param_\policy)}{\goal_{t+f}}{*}{}
      \ge \fwargs{\state_{t}}{\act_t}{\goal_{t+f}}{*}{},
\end{align}%
% 
where $\policy_*$ is the optimal goal-conditioned policy and $\act_t$ represents
the action taken at time $t$. Taking cue from \citet{MnKaSiNATURE2015}, we do
not repeat the main online network $\Q_*$ in the loss term. We use target
network $\Q_t$ and split the constraint into two loss terms. One loss term acts
as a lower bound $\LossLo$ and the other acts as the upper bound $\LossUp$:
%
\begin{align}
  \LossLo &= (
      \fwargs{\state_{t}}{\act_t}{\goal_{im}}{t}{}
      + \fwargs{\state_{im}}{\policy_t(\state_{im}, \goal_{t+f};\param_\policy)}{\goal_{t+f}}{t}{}
      - \fwargs{\state_{t}}{\act_t}{\goal_{t+f}}{*}{}
      )_+^2
                         \\
  \LossUp &= (
      \fwargs{\state_{t}}{\act_t}{\goal_{im}}{*}{}
      + \fwargs{\state_{im}}{\policy_t(\state_{im}, \goal_{t+f};\param_\policy)}{\goal_{t+f}}{t}{}
      - \fwargs{\state_{t}}{\act_t}{\goal_{t+f}}{t}{}
      )_+^2,
\end{align}%
% 
where $(\dots)_+$ denotes ReLU function.
Note that the above terms differ only by choice of target network $\fw_t$ and
main network $\fw_*$. 

Similar to HER we get the intermediate state by trajectory sampling. Once,
a transition $(\state_t, \act_t, \rew_t, \state_{t+1})$ and a future goal
$\goal_{t+f}$ has been sampled from the same episode, we sample another
intermediate state and goal pair $(\state_{im}, \goal_{im})$ such that $t \le im
\le t + f$.
% 
% \begin{align}
%   \LossTrieq &= |
%       \fwargs{\state_{b}}{\act_b}{\goal_s}{t}{}
%       + \fwargs{\state_{s}}{\policy_t(\state_s, \goal_{b};\param_\policy)}{\goal_b}{t}{}
%       - \fwargs{\state_{b}}{\act_b}{\goal_b}{m}{}
%       |^2
% \end{align}

The resulting algorithm in shown in Alg~\ref{alg:floyd-warshall-deep}.
% An
%ablation of loss functions is shown in Figure~\ref{fig:fig:fwrl-stepfwrl-noop-FetchPush}.

\input{algorithm-her-pr}
