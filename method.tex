\section{Problem definition}

%\subsection{Environment Setup}
Consider an agent interacting with an environment, $\varepsilon$. At
every time step, $t$, the agent observes a state, $\state_t \in \State$,
where $\State$ is the observation state space. The agent can traverse
the state space by taking actions, $\act \in \Act$, where $\Act$ is a
fixed action space. A goal state, $\goal \in \State$, is specified to
the agent where the goal state is a specific observation in the state
space.  $\Rgoal$ is the reward recieved by the agent for finding the
goal state and constitutes the largest reward in the environment.  For
every time step $t$, the agent takes an action $\act_t$, observes a
state $\state_t \in \State$ and receives a reward $\rew_t \in [-\Rgoal,
\Rgoal]$.  Episodes are of a fixed number of time steps, $T$. For every
episode, a randomized goal state is provided to the agent as input. If
the goal state is observed by the agent during the course of an episode,
the agent is randomly reinitialized within the environment while the
goal state remains unchanged.

As is typical in RL domains, the agent's objective is to find the
sequence of actions to take that maximizes the total reward from episode
to episode. Since the environment itself is static, this is best
achieved via the agent first discovering the goal location and then
traversing the shortest path to it from every subsequent \emph{spawn}
state during the course of an episode. The agent is best suited by
algorithms that emphasize the  transfer \emph{environment structure}
from episode to episode. 


%Once the agent reaches the goal $\|\state_t - \goal\| <
%\delta$

%
%\begin{align}
%\policy^*(\state_t ; \goal) = \act^*_t = \arg \max_{\act_t} \E_{\policy}\left[ \sum_{t=0}^T \rew_t \right]
%\end{align}%
%

%\subsection{Why is this problem important?}
Many real world problems can be formulated in this context. Consider a robot
who has moved into a new city.
The salesman has to explore the city and find the buildings that match the given
address. The next time the postman gets the same address, they can use their
experience to find out the building. Even when a new address is provided in the
next episode, the postman can use experience to find the new address in shorter
time.

In robotics, tasks like picking and placing the object at a desired
location can be formulated as goal-directed navigation.

%\subsection{Why is the problem hard?}
Model-free Reinforcement learning methods assume that the rewards are
being sampled from the a static reward function.  In a problem where the
goal location changes, hence the reward function also changes, it
becomes hard to transfer the learned value-function or action-value
function to the changed location.  One alternative is to concatenate the
goal location the state, making the new state space $[\state_t,
\goal]^\top \in \State^2$ larger.  This method is wasteful in
computation and more importantly in sample complexity.

\section{Method}
We present a model-free reinforcement learning method that easily transfers the
learned behavior when goal location is dynamic. We accomplish this by
maintaining a path based expected reward function from any state to any goal
state. We call this algorithm Floyd-Warshall Reinforcement Learning, because of
its similarity to Floyd-Warshall algorithm : a shortest-path planning algorithm
on graphs. We define Floyd-Warshall value function (FW) as
%
\begin{align}
\fwargs{\state}{\act}{\state'}{\policy}{} =
\E_{\policy}\left[ \sum_{t=0}^{t=k} \rew_t \middle\vert \state_0 = \state, \act_0 = \state, \state_k = \state' \right] ,
\end{align}%
%
where $\policy: \State \times \Action \rightarrow \Delta$ is the
stochastic policy being followed.

What is the relationship between Q-function and FW-function?
\begin{align}
  \Q_\policy(\state, \act) = \sum_{\state'} P_\policy(\state' | \state, \act) \fwargs{\state}{\act}{\state'}{\policy}{},
\end{align}%
%
where $P_\policy(\state' | \state, \act)$ is the probability of the agent
arriving at $\state'$ within the episode.

The optimal FW-function is defined as
\begin{align}
\fwargs{\state}{\act}{\state'}{\policy^*_{\state'}}{*} =
\max_{\policy_{\state'}}\E_{\policy_{\state'}}\left[ \sum_{t=0}^{t=k} \rew_t \middle\vert \state_0 = \state, \act_0 = \state, \state_k = \state' \right] ,
\end{align}%
where $\policy^*_{\state'}: \State \times \Action \rightarrow \Delta$ is the
optimal policy towards the goal state $\state'$. The
optimal Q-function and FW-function are equal same goal
  $\Q^*_{\policy^*_{\state_g}}(\state, \act) =
  \fwargs{\state}{\act}{\state_g}{\policy^*_{\state_g}}{*}$, as long as they are
  following the policy towards the same goal.

When the policy is optimal, the Floyd-Warshall function must satisfy the constraint
%
\begin{align}
\fwargs{\state_i}{\act_i}{\state_j}{\policy^*_{\state_j}}{*}
 = \sup_{\state_k} \left[
  \fwargs{\state_i}{\act_i}{\state_k}{\policy^*_{\state_k}}{*}
  + \max_{\act_k}\fwargs{\state_k}{\act_k}{\state_j}{\policy^*_{\state_j}}{*} \right] ,
\end{align}%
%
which states that the optimal FW function from a given start state to a
given goal state should be greater than or equal to the summation of FW
function via any intermediate state. 

We summarize the algorithm in Alg~\ref{alg:floyd-warshall-small}.

\TODO{Should we use Q-function as backdrop?}


\input{algorithm}

