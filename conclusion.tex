\section{Conclusion}
In this work we pose a reinterpretation of goal-conditioned value
functions and show that under this paradigm learning is possible in the
absence of goal reward. This is a suprising result that runs counter
to intuitions that underly most reinforcement learning algorithms.  

Since our method cannot recognize locations close to the desired
location, in future work we wish to extend our formulation to
incorporate the distance threshold. We also wish to improve the
the performance of FWRL on deep neural networks. 

We hope that the experiments and results presented in this paper  lead
to a broader discussion about the assumptions actually required for
learning multi-goal tasks.
