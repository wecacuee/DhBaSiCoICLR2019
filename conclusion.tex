\section{Conclusion}
In this work we pose a reinterpretation of goal-conditioned value
functions and show that under this paradigm learning is possible in the
absence of goal reward. This is a surprising result that runs counter
to intuitions that underlie most reinforcement learning algorithms.  
In future work, we will augment our method to incorporate the
distance-threshold information to make the task easier to learn when the
threshold is high. 
We hope that the experiments and results presented in this paper lead
to a broader discussion about the assumptions actually required for
learning multi-goal tasks.
