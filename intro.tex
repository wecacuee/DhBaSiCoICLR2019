%From playing Atari games from raw pixel values \citep{mnih2013playing},
%beating world GO champions \citep{silver2017mastering} and applications
%to robotic navigation \citep{zhu2017target} and manipulation
%\citep{gu2017deep}, the accomplishments of Deep Reinforcement Learning
%(DRL) have been wide and varied. Of interest to this work has been the
%recent application of these powerful algorithms to complicated
%multi-goal tasks \citep{andrychowicz2017hindsight,pong2018temporal}. In
%Multi-Goal Reinforcement Learning (MGRL)\citep{plappert2018multi},
%agents are tasked with learning policies that can traverse to dynamic,
%previously unseen goal locations. Many tasks can be interpreted in
%multi-goal domains.  For example, a robotic arm may need to move objects
%to given \emph{positions} around a table. In another example, a mobile
%robot may be tasked with navigating to given \emph{landmarks}.

\section{Introduction}

% Reinforcement learning and it's uses
% Why we care about Multi-Goal Reinforcement Learning
Modern robots are able to handle complex tasks involving sequences of
goals, called \textit{multi-goal tasks}.  For example, a robotic arm may
need to move objects to given positions around a table
\citep{gu2017deep}. A mobile robot may be tasked with navigating to
given landmarks in a map \citep{zhu2017target}. Deep Reinforcement
Learning algorithms have been adapted to work on such problems showing
promising results \citep{andrychowicz2017hindsight,pong2018temporal}. 

% A common way to solve MGRL problems is the use of Goal-Conditioned
% Value functions. Solving several tasks instead of individual tasks
Multi-Goal Reinforcement Learning (MGRL) \citep{plappert2018multi}
algorithms generally work by estimating \emph{goal-conditioned value
functions} (GCVF). A GCVF, $V(s,g)$, is a specialized \emph{value
function} that operates on both state and goal space. These functions
represent the expected cumulative reward of reaching any goal from any
start state and are used to compute \emph{policies} for multi-goal
tasks. 

% Goal conditioned value functions make a big assumption however. It is
% assumed that to learn to achieve goals, goal locations must themselves
% must be associated with reward. This however is not the case. We
% introduce ....
State-of-the art algorithms for learning GCVFs
\cite{andrychowicz2017hindsight, pong2018temporal} operate under two
broad assumptions. The first is the requirement of \emph{goal-rewards}
i.e. the dependence of environment reward
functions on the desired goal location, $r(s,a,g)$.
For example, in the Fetch-Push task \citep{plappert2018multi} of moving
a block to a given location on a table, every movement incurs a ``-1''
reward while on reaching the desired goal, ``0'' reward is recieved.  To
achieve maximum reward, the agent is incentivized to traverse and remain
in the goal in a minimal number of steps.  The second,
\emph{reward-resampling} is that these
reward functions can resampled independent of trial execution. 
This is used to relabel past experiences as succesful that has the
effect of accelerating learning. 

% Do goal-conditioned value functions require goal rewards to learn?
% Describe some form of the intuition
While these methods are making progress for MGRL problems, the use of
these assumptions incur expense and are limitations. In this work, we
propose a new interpretation of \emph{goal-conditioned value functions}
that do not require neither the specification of goals or reward resampling.
This interpration when applied to state-of-the-art methods leads to
equal performance across several metrics while substantially reducing sample
complexity when rewarding-resampling is taken in to account. Running
counter to most RL paradigms, we show that \emph{goal-rewards} are not
required to learn GCVFs.  

% Detail the intuition and the actual contribution
\TODO{Important paragraph}
The intuition driving this work is the idea of 
The key contribution is the re-interpretation of GCVFs to
be the cumulative \emph{path reward} of going from a given start state
to a given goal state.  This is different than the conventional
interpretation of GCVFs as \emph{future rewards} from a given start
state to any state using the goal state as additional information.
Since we are learning the GCVF for all possible goal states, we need not
treat goal-reward in a specialized manner. The specification of the goal
is utilized at the policy computation step from the GCVF, rather than
the learning stage. This \emph{path-reward} interpretation requires an
additional loss term in the optimization, called the \emph{step-loss},
that learns GCVFs when the goal is only one step away. 

% Detail the Floyd-Warshall contribution
A secondary contribution of this work is the extension of
\emph{Floyd-Warshall Reinforcement Learning} (FWRL)
\citep{dhiman2018floydwarshall} to DRL.  FWRL is a form of RL that
leverages compositionality constraints in the space of GCVFs to learn.
Experimentation in the original work was limited to tabular domains. In
this work, we extend FWRL to deep domains by introducing additional
terms to the \emph{loss} that are inspired by these compositionality
constraints. 

% Summarize 
In summary, the  contributions of this work are twofold. Firstly, we
reinterpret goal-conditioned value functions as expected path-rewards
thereby removing the dependency on \emph{goal-rewards} and
\emph{reward-resampling}.
We showcase how the application of this interpreation leads to improved
learning. Secondly, we extend Floyd-Warshal Reinforcement Learning from
tabular domains to the realm of DRL \cite{}. 


%% There is inherent structure in the formulation of these functions
%% thathas been ignored. Dhiman et al introduced structure in the space
%% of these functions 
%Floyd-Warshall Reinforcement Learning was first introduced in Dhiman et.
%al. for tabular domains. 
%The FW functions is defined 
%%
%\begin{align}
%\fwargs{\state}{\act}{\state'}{\policy}{} =
%\E_{\policy}\left[ \sum_{t=0}^{t=k} \rew_t \middle\vert \state_0 = \state, \act_0 = \state, \state_k = \state' \right] .
%\end{align}%
%%
%which is the expected sum of rewards when the state is s and the
%end state is the goal. Compositionality constraints are then used to
%learn the FW function.
%%
%\begin{align}
%\fwargs{\state_i}{\act_i}{\state_j}{\policy^*_{\state_j}}{*}
% \ge 
%  \fwargs{\state_i}{\act_i}{\state_k}{\policy^*_{\state_k}}{*}
%  + \max_{\act_k}\fwargs{\state_k}{\act_k}{\state_j}{\policy^*_{\state_j}}{*}
%  ,\forall \state_k \in \State.
%  \label{eq:fwconstraint}
%\end{align}%
%%
%The constraint is inspired by the Floyd-Warshall algorithm for path
%planning on graphs and states that the expected reward in traversing
%from one state to another should be greater than or equal to that of the
%paths through intermediary states.  While Dhiman et al. was restritcted
%to toy examples in the gridworld doman and tabular functions, they are
%not naturally scalabale to real life problems. This work extends their
%formulation to paramteric functions like neural networks by addition
%terms to the RL objective based on these compositionality constraints.
%Since we extend the algorithm to the Deep Learning domain, we call it
%Deep Floyd-Warshall Reinforcement Learning. 






