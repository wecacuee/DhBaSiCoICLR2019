\section{Introduction}

% Reinforcement learning and it's uses
% Why we care about Multi-Goal Reinforcement Learning
Many tasks in robotics require the specification of a \emph{goal}. For
example, a robotic arm may need to move an object to a goal position on
a table \citep{gu2017deep}. A mobile robot may be tasked with navigating
to a goal landmark in a map \citep{zhu2017target}.  The adaptation of
reinforcement learning to such goal-conditioned tasks is called
Multi-Goal Reinforcement Learning (MGRL)\citep{plappert2018multi}. 

% A common way to solve MGRL problems is the use of Goal-Conditioned
% Value functions. Solving several tasks instead of individual tasks
State-of-the-art MGRL algorithms
\citep{andrychowicz2017hindsight, pong2018temporal}
work by estimating \emph{goal-conditioned value functions} (GCVF). A
GCVF is defined as the expected cumulative reward of reaching from a
start state conditioned on the given goal state. They are used to compute the
actions to be taken at any state, also known as the \emph{policy}.

% Goal conditioned value functions make a big assumption however. It is
% assumed that to learn to achieve goals, goal locations must themselves
% must be associated with reward. This however is not the case. We
% introduce ....
These algorithms require the use of \emph{goal-rewards} which is the
dependence of reward functions on the desired goal, $r(s,a,g)$.
For example, in the Fetch-Push task \citep{plappert2018multi} of moving
a block to a given location on a table, every movement incurs a ``-1''
reward while reaching the desired goal earns ``0'' reward. Althought
this dependence allows for the association of high reward with the
reaching of goals, it incurs additional costs. In Hindsight Experience
Replay (HER) \citep{andrychowicz2017hindsight}, failed experiences are
re-evaluated as succesful ones by assuming traversed states to be
desired goals. Due to the above dependence, the association of every
pseudo-goal with high reward requires an independent
\emph{reward-recomputation} step. 

%This leads to the
%requirement of \emph{reward-recomputation}, which is that these reward functions can be
%computed indendent of \emph{environment} interaction.  The second assumption enables a technique called \emph{Hindsight
%Experience Replay} (HER) that accelerates learning 

% Do goal-conditioned value functions require goal rewards to learn?
% Describe some form of the intuition
The goal-rewards requirement is avoidable. Consider a
student who has moved to a new university. To learn about the campus,
the student explores it randomly with no specific goal in mind. When
tasked with finding a goal classroom, the student can then use the
learned connectivity of the campus to make their way to class in the
shortest possible explored path. The key intuition here is that the
student is not incentivized to explore specific goal locations (i.e. no
goal rewards). 
%
Based on this intuition of goal-less learning, we redefine GCVFs to be
the expected \emph{path-reward} that is learned for all possible
start-goal pairs. Instead of goal rewards, we  
introduce \emph{one-step loss} that assumes single steps paths 
to be the maximum reward paths between adjacent pairs.
Under this interpretation, the \emph{Bellman equation} chooses and chains
together one-step paths to find longer maximum reward paths. 

%
Experimentally, we show how this simple reinterpration achieves equally
good performance as a representative state-of-the-art method (HER)
without using goal-rewards and vastly outperforms it in terms of
reward sample complexity.

% Detail the Floyd-Warshall contribution
In this work, we also extend Floyd-Warshall Reinforcement
Learning (FWRL) \citep{dhiman2018floydwarshall} to the use paramteric
function approximators instead of tabular functions.  We leverage FWRL's
compositionality constraints in the space of GCVFs to introduce
additional loss terms to the objective. However, these additional loss
terms do not show improvement over the baseline. We conjecture that the
compositionality constraints are already captured by other loss terms. 

% Summarize 
In summary, the  contributions of this work are twofold. Firstly, we
reinterpret goal-conditioned value functions as expected path-rewards
and introduce one-step loss thereby removing their dependency on
goal-rewards and reward-resampling. We showcase how the
application of this interpreation leads to improved reward sample complexity.
Secondly, we extend the tabular Floyd-Warshal Reinforcement Learning to
use deep neural networks.


%% There is inherent structure in the formulation of these functions
%% thathas been ignored. Dhiman et al introduced structure in the space
%% of these functions 
%Floyd-Warshall Reinforcement Learning was first introduced in Dhiman et.
%al. for tabular domains. 
%The FW functions is defined 
%%
%\begin{align}
%\fwargs{\state}{\act}{\state'}{\policy}{} =
%\E_{\policy}\left[ \sum_{t=0}^{t=k} \rew_t \middle\vert \state_0 = \state, \act_0 = \state, \state_k = \state' \right] .
%\end{align}%
%%
%which is the expected sum of rewards when the state is s and the
%end state is the goal. Compositionality constraints are then used to
%learn the FW function.
%%
%\begin{align}
%\fwargs{\state_i}{\act_i}{\state_j}{\policy^*_{\state_j}}{*}
% \ge 
%  \fwargs{\state_i}{\act_i}{\state_k}{\policy^*_{\state_k}}{*}
%  + \max_{\act_k}\fwargs{\state_k}{\act_k}{\state_j}{\policy^*_{\state_j}}{*}
%  ,\forall \state_k \in \State.
%  \label{eq:fwconstraint}
%\end{align}%
%%
%The constraint is inspired by the Floyd-Warshall algorithm for path
%planning on graphs and states that the expected reward in traversing
%from one state to another should be greater than or equal to that of the
%paths through intermediary states.  While Dhiman et al. was restritcted
%to toy examples in the gridworld doman and tabular functions, they are
%not naturally scalabale to real life problems. This work extends their
%formulation to paramteric functions like neural networks by addition
%terms to the RL objective based on these compositionality constraints.
%Since we extend the algorithm to the Deep Learning domain, we call it
%Deep Floyd-Warshall Reinforcement Learning. 






