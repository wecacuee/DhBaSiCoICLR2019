%From playing Atari games from raw pixel values \citep{mnih2013playing},
%beating world GO champions \citep{silver2017mastering} and applications
%to robotic navigation \citep{zhu2017target} and manipulation
%\citep{gu2017deep}, the accomplishments of Deep Reinforcement Learning
%(DRL) have been wide and varied. Of interest to this work has been the
%recent application of these powerful algorithms to complicated
%multi-goal tasks \citep{andrychowicz2017hindsight,pong2018temporal}. In
%Multi-Goal Reinforcement Learning (MGRL)\citep{plappert2018multi},
%agents are tasked with learning policies that can traverse to dynamic,
%previously unseen goal locations. Many tasks can be interpreted in
%multi-goal domains.  For example, a robotic arm may need to move objects
%to given \emph{positions} around a table. In another example, a mobile
%robot may be tasked with navigating to given \emph{landmarks}.

\section{Introduction}

% Reinforcement learning and it's uses
% Why we care about Multi-Goal Reinforcement Learning
Many tasks in robotics, called \emph{multi-goal tasks},  need the
specification of a \emph{goal-state}.  For example, a robotic arm may
need to move objects to given positions around a table
\citep{gu2017deep}. A mobile robot may be tasked with navigating to
given landmarks in a map \citep{zhu2017target}.  Deep
Reinforcement Learning algorithms have been adapted to work on such
problems showing promising results \citep{andrychowicz2017hindsight,pong2018temporal}. 

% A common way to solve MGRL problems is the use of Goal-Conditioned
% Value functions. Solving several tasks instead of individual tasks
Multi-Goal Reinforcement Learning (MGRL) \citep{plappert2018multi}
algorithms generally work by estimating \emph{goal-conditioned value
functions} (GCVF). A GCVF, is an extension to \emph{value functions} in
reinforcement learning that in addition to operating on state space,
also operates over goal space.  These functions represent the expected
cumulative reward of reaching any goal from any start state and are used
to compute \emph{policies} for multi-goal tasks. 


% Goal conditioned value functions make a big assumption however. It is
% assumed that to learn to achieve goals, goal locations must themselves
% must be associated with reward. This however is not the case. We
% introduce ....
\TODO{Fill in the related works}\\
Many algorithms have been proposed to learn goal-conditioned value
functions. In \emph{Universal Value Function Approximators}, \cite{} ...
In \emph{Hindsight Experience Replay} ... In \emph{Temporal Difference
Models} ... \emph{Reward} is conventionally a function of the agent's
action and its state, $r(s,a)$. All these works build of the assumption
that it is also conditioned on the desired goal, $r(s,a,g)$ thereby
introducing an extra dimension in the formulation. . For example, in the
Fetch-Push task \citep{plappert2018multi} of moving a block to a given
location on a table, every movement incurs a ``-1'' reward while on
reaching the desired goal, ``0'' reward is recieved.  To achieve maximum
reward, the agent is incentivized to traverse and remain in the goal
in a minimal number of steps.  Could GCVFs still be learned if no
special reward was recieved on reaching the goal?  In this work,
we pose the question: \emph{``Are goal-rewards really needed to learn
goal-conditioned value functions?''}. 

% Do goal-conditioned value functions require goal rewards to learn?
\TODO{Rewrite this paragraph with Jason's help}\\
Counter-intuitively, we answer this question in the negative.  The key
contribution is the re-interpretation of GCVFs to be the cumulative
\emph{path reward} of going from a given start state to a given goal
state.  This is different than the conventional interpretation of GCVFs
as \emph{future rewards} from a given start state to any state using the
goal state as additional information.  
Since we are learning the GCVF for all possible goal states, we need not treat
goal-reward in a specialized manner. The specification of the goal is
utilized at the policy computation step from the GCVF, rather than the
learning stage. This \emph{path-reward} interpretation requires an
additional loss term in the optimization, called the
\emph{step-loss}, that learns GCVFs when the goal is only one step away. 

% Do goal-conditioned value functions require goal rewards to learn?
Experimentally, we showcase how this reinterpretation when applied to
HER and a deep adaptation of Floyd-Warshall Reinforcement Learning
\citep{}, substantially improves sample efficiency while reducing asymptotic distance to
the goal across a variety of challenging environments. 

% In summary, our contributions are and results show increased sample
% efficiency. 
In summary, the  contributions of this work are twofold. Firstly, we
reinterpret goal-conditioned value functions as expected path-rewards
thereby removing the dependency of the reward function on goal states.
We showcase how the application of this interpreation leads to improved
learning. Secondly, we extend Floyd-Warshal Reinforcement Learning from
tabular domains to the realm of DRL \cite{}. 


%% There is inherent structure in the formulation of these functions
%% thathas been ignored. Dhiman et al introduced structure in the space
%% of these functions 
%Floyd-Warshall Reinforcement Learning was first introduced in Dhiman et.
%al. for tabular domains. 
%The FW functions is defined 
%%
%\begin{align}
%\fwargs{\state}{\act}{\state'}{\policy}{} =
%\E_{\policy}\left[ \sum_{t=0}^{t=k} \rew_t \middle\vert \state_0 = \state, \act_0 = \state, \state_k = \state' \right] .
%\end{align}%
%%
%which is the expected sum of rewards when the state is s and the
%end state is the goal. Compositionality constraints are then used to
%learn the FW function.
%%
%\begin{align}
%\fwargs{\state_i}{\act_i}{\state_j}{\policy^*_{\state_j}}{*}
% \ge 
%  \fwargs{\state_i}{\act_i}{\state_k}{\policy^*_{\state_k}}{*}
%  + \max_{\act_k}\fwargs{\state_k}{\act_k}{\state_j}{\policy^*_{\state_j}}{*}
%  ,\forall \state_k \in \State.
%  \label{eq:fwconstraint}
%\end{align}%
%%
%The constraint is inspired by the Floyd-Warshall algorithm for path
%planning on graphs and states that the expected reward in traversing
%from one state to another should be greater than or equal to that of the
%paths through intermediary states.  While Dhiman et al. was restritcted
%to toy examples in the gridworld doman and tabular functions, they are
%not naturally scalabale to real life problems. This work extends their
%formulation to paramteric functions like neural networks by addition
%terms to the RL objective based on these compositionality constraints.
%Since we extend the algorithm to the Deep Learning domain, we call it
%Deep Floyd-Warshall Reinforcement Learning. 






