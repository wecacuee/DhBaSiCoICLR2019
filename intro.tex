\section{Introduction}

% Reinforcement learning and it's uses
% Why we care about Multi-Goal Reinforcement Learning
Many tasks in robotics require the specification of a \emph{goal} for every
trial. For example, a robotic arm may need to move an object to an arbitrary
goal position on a table \citep{gu2017deep}; a mobile robot may need to with
navigating to a arbitrary goal landmark on a map \citep{zhu2017target}. The
adaptation of reinforcement learning to such goal-conditioned tasks where goal
locations can change is called Multi-Goal Reinforcement Learning
(MGRL)~\citep{plappert2018multi}.

% A common way to solve MGRL problems is the use of Goal-Conditioned
% Value functions. Solving several tasks instead of individual tasks
State-of-the-art MGRL algorithms
\citep{andrychowicz2017hindsight, pong2018temporal}
work by estimating \emph{goal-conditioned value functions} (GCVF). A
GCVF is defined as the expected cumulative reward from a
start state with a specified the goal. It is used to compute the
actions to be take at any state, also known as the \emph{policy}.

% Goal conditioned value functions make a big assumption however. It is
% assumed that to learn to achieve goals, goal locations must themselves
% must be associated with reward. This however is not the case. We
% introduce ....
These algorithms require the use of \emph{goal-rewards}, which is the
dependence of reward functions on the desired goal, $r(s,a,g)$.
For example, in the Fetch-Push task \citep{plappert2018multi} of moving
a block to a given location on a table, every movement incurs a ``-1''
reward while reaching the desired goal induces ``0'' reward. Although
this dependence allows for the agent to associate reaching the goal with a high
reward, it may incur additional sampling costs. For example, in Hindsight Experience
Replay (HER) \citep{andrychowicz2017hindsight}, failed experiences are
re-evaluated as succesful ones by assuming traversed states to be
desired pseudo-goals. Due to the dependence of reward function on the goal,
the association of every pseudo-goal with high reward requires an independent
\emph{reward-recomputation} step. 

%This leads to the
%requirement of \emph{reward-recomputation}, which is that these reward functions can be
%computed indendent of \emph{environment} interaction.  The second assumption enables a technique called \emph{Hindsight
%Experience Replay} (HER) that accelerates learning 

% Do goal-conditioned value functions require goal rewards to learn?
% Describe some form of the intuition
The goal-rewards requirement is avoidable. Let us consider an example to
motivate why goal-rewards may be avoided in goal-conditioned tasks. Consider a
student who has moved to a new university. To learn about the campus,
the student explores it randomly with no specific goal in mind. When
tasked with finding a goal classroom, the student can then use the
learned connectivity of the campus to make their way to class along the
shortest explored path. The key intuition here is that the
student is not incentivized to explore specific goal locations (i.e. no
goal rewards). 
%
Based on this intuition of goal-less learning, we redefine GCVFs to be
the expected \emph{path-reward} that is learned for all possible
start-goal pairs. Instead of goal rewards, we  
introduce a \emph{one-step loss} that assumes single steps paths 
to be the maximum reward paths between adjacent pairs.
Under this interpretation, the \emph{Bellman equation} chooses and chains
together one-step paths to find longer maximum reward paths. 

%
Experimentally, we show how this simple reinterpration, which does not use goal
rewards, performs as well as a representative state-of-the-art method (HER) and
even outperforms it in terms of reward sample complexity.

% Detail the Floyd-Warshall contribution
In this work, we also extend a closely related algorithm, Floyd-Warshall Reinforcement
Learning (FWRL) \citep{dhiman2018floydwarshall} to use parametric
function approximators instead of tabular functions. Similar to our
re-definition of GCVFs, FWRL learn a goal-conditioned Floyd-Warshall function
that represents path-rewards instead of future-rewards.
We translate FWRL's compositionality constraints in the space of GCVFs to introduce
additional loss terms to the objective. However, these additional loss
terms do not show improvement over the baseline. We conjecture that the
compositionality constraints are already captured by other loss terms. 

% Summarize 
In summary, the  contributions of this work are twofold. Firstly, we
reinterpret goal-conditioned value functions as expected path-rewards
and introduce one-step loss thereby removing their dependency on
goal-rewards and reward-resampling. We showcase how the
application of this interpreation leads to improved sample efficiency in  terms
of reward samples.
Secondly, we extend the tabular Floyd-Warshal Reinforcement Learning to
use deep neural networks.


%% There is inherent structure in the formulation of these functions
%% thathas been ignored. Dhiman et al introduced structure in the space
%% of these functions 
%Floyd-Warshall Reinforcement Learning was first introduced in Dhiman et.
%al. for tabular domains. 
%The FW functions is defined 
%%
%\begin{align}
%\fwargs{\state}{\act}{\state'}{\policy}{} =
%\E_{\policy}\left[ \sum_{t=0}^{t=k} \rew_t \middle\vert \state_0 = \state, \act_0 = \state, \state_k = \state' \right] .
%\end{align}%
%%
%which is the expected sum of rewards when the state is s and the
%end state is the goal. Compositionality constraints are then used to
%learn the FW function.
%%
%\begin{align}
%\fwargs{\state_i}{\act_i}{\state_j}{\policy^*_{\state_j}}{*}
% \ge 
%  \fwargs{\state_i}{\act_i}{\state_k}{\policy^*_{\state_k}}{*}
%  + \max_{\act_k}\fwargs{\state_k}{\act_k}{\state_j}{\policy^*_{\state_j}}{*}
%  ,\forall \state_k \in \State.
%  \label{eq:fwconstraint}
%\end{align}%
%%
%The constraint is inspired by the Floyd-Warshall algorithm for path
%planning on graphs and states that the expected reward in traversing
%from one state to another should be greater than or equal to that of the
%paths through intermediary states.  While Dhiman et al. was restritcted
%to toy examples in the gridworld doman and tabular functions, they are
%not naturally scalabale to real life problems. This work extends their
%formulation to paramteric functions like neural networks by addition
%terms to the RL objective based on these compositionality constraints.
%Since we extend the algorithm to the Deep Learning domain, we call it
%Deep Floyd-Warshall Reinforcement Learning. 






