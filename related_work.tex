
\section{Related work}
%\subsection{Goal-conditioned value functions}

\paragraph{Goal-conditioned value functions}
Multi-goal reinforcement learning has gained some attention lately with
important work like UVF~\citep{schaul2015universal},
HER~\citep{andrychowicz2016learning}, TDM~\citep{pong2018temporal} making
progress towards learning goal-conditioned value functions that help in
addressing multi-goal tasks. Universal goal-conditioned value functions
(UVFs)~\citep{schaul2015universal}, $V(s,g)$ measures the utility function of
achieving any goal from any state. UVFs are learned using traditional Q-learning
based approaches coupled with matrix factorization based methods for faster
learning.
\citet{andrychowicz2016learning} introduced the idea of Hindsight
Experience Replay (HER) to learn about the model from previous episodes
even when the goal location has changed or not been achieved.  Their
method works by utilizing experience from episodes where agent failed to reach
the goal re-imagining the last state to be the goal state.
\citet{pong2018temporal} propose Temporal Difference Models (TDM) that estimate
goal directed horizon dependent value function $Q(s, a, g, \tau)$. There work is
limited because they assume that rewards are available densely in the form of
some distance like measure from the goal.
In contrast to all the above works our contribution is a novel algorithm for
learning UVFs via leveraging a triangular-inequality like constraint within the
space of these functions.

Much recent work has explored the application of deep-reinforcement
learning for use in navigation domains. Agent's are usually initialized
in complicated 3D environments and tasked with finding goals in them
using only monocular visual videos as input. Of interest to this work is
the finding of static goals in dynamic environments, which can be seen
as a specialized instance of the generic reinforcement learning domains
this work addresses. In \citet{mirowski2016learning}, agent's in statc
environments are found to navigate to both static and dynamic goal
locations.  \citet{dhiman2018critical} showcases that in the case of
dynamic goal locations, the path chosen to reach the goal are often far
from optimal. In \citet{gupta2017cognitive},  agent's are shown to be
able to perform this navigation but their explicit utilization of
navigation based data-structures calls in to question the
generalizability of their method to other goal-conditioned reinforcement
learning domains. In \cite{savinov2018semi}, the navigation tasks is
explicitly solved via the creation of a navigation graph and the
utilization of Djikstra's algorithm. While agent's are able to
generalize learned behaviors, they do not use reinforcement learning and
are not end-to-end. In Neural Map, I'm sure there are other critiques
too. There are many other navigation works that do not consider dynamic
goals in static environments. 

Model-based approaches are known to have lower asymptotic performance
then model-free approaches. More recently, model-based algorithms have
shown more promise by explicitly modelling uncertainty. However this is
still an active area of research \cite{lakshminarayanan2017simple,
kurutach2018model,zhang2018solar}.

%FWRL is an algorithm for learning universal goal-conditioned value
%functions, first introduced in \citet{schaul2015universal}. A universal
%goal condition value function, $V(s, g)$ is a value function that takes
%both the state and the desired goal-location as input. While the orignal
%work learns \emph{UVFA}s using traditional Q-Learning, our contribution
%is a novel algorithm for learing UVFAs using path-planning based
%constraints. 
%
%The idea of goal-conditioned value function is not new but has got
%attention because of revival of reinforcement learning based on deep
%neural networks.  We build upon the goal-conditioned value functions of
%\citet{schaul2015universal}.  \citet{schaul2015universal} proposed an
%architecture and a matrix factorization based algorithm for faster
%learning of UVFA (Universal value function approximators).  UVFA focused
%on fast estimation of goal-conditioned value functions using sparse
%matrix factorization but not on bridging the gap between model-based and
%model-free algorithms.



%\subsection{Model free DRL }
%does not generalize to multi-goal environments.
%
%\subsection{Model based DRL}
%Needs more exploration.
%Find the paper that shows that Model based DRL can actually compete with Model free DRL as long as it models uncertainty.
%
%\subsection{Multi-goal navigation based papers}
%Mirowski 2017, 2018: No one shot map learning, does not generalizes to new maps.
