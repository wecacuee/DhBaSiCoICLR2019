
\section{Related work}
%\subsection{Goal-conditioned value functions}

\paragraph{Goal-conditioned value functions}
Multi-goal reinforcement learning has gained some attention lately with
important work like UVF~\citep{schaul2015universal},
HER~\citep{andrychowicz2016learning}, TDM~\citep{pong2018temporal} making
progress towards learning goal-conditioned value functions that help in
addressing multi-goal tasks. Universal goal-conditioned value functions
(UVFs)~\citep{schaul2015universal}, $V(s,g)$ measures the utility function of
achieving any goal from any state. UVFs are learned using traditional Q-learning
based approaches coupled with matrix factorization based methods for faster
learning.
\citet{andrychowicz2016learning} introduced the idea of Hindsight
Experience Replay (HER) to learn about the model from previous episodes
even when the goal location has changed or not been achieved.  Their
method works by utilizing experience from episodes where agent failed to reach
the goal re-imagining the last state to be the goal state.
\citet{pong2018temporal} propose Temporal Difference Models (TDM) that estimate
goal directed horizon dependent value function $Q(s, a, g, \tau)$. There work is
limited because they assume that rewards are available densely in the form of
some distance like measure from the goal.
In contrast to all the above works our contribution is a novel algorithm for
learning UVFs via leveraging a triangular-inequality like constraint within the
space of these functions.

\paragraph{Goal directed visual navigation}
There has been considerable interest in using Deep Reinforcement Learning (DRL)
algorithms for goal-driven visual navigation of
robots~\citep{mirowski2016learning,MiPaViICLR2017,dhiman2018critical,gupta2017cognitive,savinov2018semi}.
\cite{mirowski2016learning} demonstrate that a DRL algorithm called Asynchronous
Advantage Actor Critic (A3C) can learn to find a goal in 3D navigation
simulators, using only first person view as the input, while
\cite{MiPaViICLR2017} demonstrate goal directed navigation in Google street view
graph. \citet{gupta2017cognitive} the goal directed navigation and mapping
depends upon a specific 2D data-structure called occupancy grid which limits the
method's generalize-ability to other robotic tasks like reaching, pick and
place. This line of work has struggled to move from simulations to real world
because high sample complexity of model-free RL algorithms. Moreover, none of
this work focuses on general goal driven tasks in RL, where goal space is same
as state space. \citet{dhiman2018critical} empirically evaluate
\cite{mirowski2016learning}'s approach and show that when goal locations are
dynamic, the path chosen to reach the goal are often far from optimal.
In contrast to our work, all the above works focus on navigation domain and
employ domain specific auxiliary rewards and data-structures rather the focusing
on the multi-goal task domain.

\paragraph{Binary goal tasks}
\citet{parisotto2017neural} extend the idea of external neural network memory
that is indexed by spatial coordinates rather then time, as done in previous
works. However, their work, in a similar vein to \cite{OhChSiICML2016},
considers goal specification as a binary signal hidden somewhere in the
observation space. In such problems the challenge is to learn the goal
specification protocol and avoid the wrong goal while exploring for the right
goal. These problems domains not evaluate the ability of agent to find the
shortest path to the goal.

\paragraph{Model-based RL}
Model-based approaches are known to have lower asymptotic performance then
model-free approaches~\cite{pong2018temporal}. In many tasks, like moving a
glass of water, it is easier to model the expected rewards rather than state
dynamics. This is because inaccurate modeling of the water-surface-dynamics
won't affect the rewards as long the agent does not spill large amounts of
water. More recently, model-based algorithms have shown more promise by
explicitly modelling uncertainty~\cite{lakshminarayanan2017simple,
  kurutach2018model,zhang2018solar}.
However, for model-based algorithms to compete with model-free algorithms, the
uncertainty in state-dynamics modeling needs to depend upon its effect on
expected rewards.


%FWRL is an algorithm for learning universal goal-conditioned value
%functions, first introduced in \citet{schaul2015universal}. A universal
%goal condition value function, $V(s, g)$ is a value function that takes
%both the state and the desired goal-location as input. While the orignal
%work learns \emph{UVFA}s using traditional Q-Learning, our contribution
%is a novel algorithm for learing UVFAs using path-planning based
%constraints. 
%
%The idea of goal-conditioned value function is not new but has got
%attention because of revival of reinforcement learning based on deep
%neural networks.  We build upon the goal-conditioned value functions of
%\citet{schaul2015universal}.  \citet{schaul2015universal} proposed an
%architecture and a matrix factorization based algorithm for faster
%learning of UVFA (Universal value function approximators).  UVFA focused
%on fast estimation of goal-conditioned value functions using sparse
%matrix factorization but not on bridging the gap between model-based and
%model-free algorithms.



%\subsection{Model free DRL }
%does not generalize to multi-goal environments.
%
%\subsection{Model based DRL}
%Needs more exploration.
%Find the paper that shows that Model based DRL can actually compete with Model free DRL as long as it models uncertainty.
%
%\subsection{Multi-goal navigation based papers}
%Mirowski 2017, 2018: No one shot map learning, does not generalizes to new maps.
