
\section{Related work}
\subsection{Goal-conditioned value functions}

Universal goal-conditioned value functions (UVFs), $V(s,g)$, described in
\citet{schaul2015universal}, measures the utility function of achieving any goal
from any state. In the original work, UVFs are learned using traditional
Q-learning based approaches coupled with matrix factorization based
methods for faster learning. In this work, our contribution is a novel
algorithm for learning UVFs via leveraging constraints within the space
of these functions. 

Other approaches have been proposed for learning UVFs.
\citet{andrychowicz2016learning} introduced the idea of Hindsight
experience replay (HER) to learn about the model from previous episodes
even when the goal location has changed or not been achieved.  Their
method works by considering failed state-traversals as succesful ones in
the course of training. Unliked our proposed algorithm, their methods
fails to take in to account relationships between learned Q-functions
between common state traversal pairs. In this regard our work, our
method can be seen as parametric approximation of ``Hindsight memory'',
that can help compress information from previous episodes instead of
maintaining the entire history of replay memory.

\cite{pong2018temporal} propose temporal difference models that estimate
goal directed Q function for a specific kind of reward function, in
particular the distance from the goal and in contrast with limited
temporal horizon.

Much recent work has explored the application of deep-reinforcement
learning for use in navigation domains. Agent's are usually initialized
in complicated 3D environments and tasked with finding goals in them
using only monocular visual videos as input. Of interest to this work is
the finding of static goals in dynamic environments, which can be seen
as a specialized instance of the generic reinforcement learning domains
this work addresses. In \citet{mirowski2016learning}, agent's in statc
environments are found to navigate to both static and dynamic goal
locations.  \citet{dhiman2018critical} showcases that in the case of
dynamic goal locations, the path chosen to reach the goal are often far
from optimal. In \citet{gupta2017cognitive},  agent's are shown to be
able to perform this navigation but their explicit utilization of
navigation based data-structures calls in to question the
generalizability of their method to other goal-conditioned reinforcement
learning domains. In \citer{savinov2018semi}, the navigation tasks is
explicitly solved via the creation of a navigation graph and the
utilization of Djikstra's algorithm. While agent's are able to
generalize learned behaviors, they do not use reinforcement learning and
are not end-to-end. In Neural Map, I'm sure there are other critiques
too. There are many other navigation works that do not consider dynamic
goals in static environments. 

Model-based approaches are known to have lower asymptotic performance
then model-free approaches. More recently, model-based algorithms have
shown more promise by explicitly modelling uncertainty. However this is
still an active area of research \cite{lakshminarayanan2017simple,
kurutach2018model,zhang2018solar}.

%FWRL is an algorithm for learning universal goal-conditioned value
%functions, first introduced in \citet{schaul2015universal}. A universal
%goal condition value function, $V(s, g)$ is a value function that takes
%both the state and the desired goal-location as input. While the orignal
%work learns \emph{UVFA}s using traditional Q-Learning, our contribution
%is a novel algorithm for learing UVFAs using path-planning based
%constraints. 
%
%The idea of goal-conditioned value function is not new but has got
%attention because of revival of reinforcement learning based on deep
%neural networks.  We build upon the goal-conditioned value functions of
%\citet{schaul2015universal}.  \citet{schaul2015universal} proposed an
%architecture and a matrix factorization based algorithm for faster
%learning of UVFA (Universal value function approximators).  UVFA focused
%on fast estimation of goal-conditioned value functions using sparse
%matrix factorization but not on bridging the gap between model-based and
%model-free algorithms.



%\subsection{Model free DRL }
%does not generalize to multi-goal environments.
%
%\subsection{Model based DRL}
%Needs more exploration.
%Find the paper that shows that Model based DRL can actually compete with Model free DRL as long as it models uncertainty.
%
%\subsection{Multi-goal navigation based papers}
%Mirowski 2017, 2018: No one shot map learning, does not generalizes to new maps.
