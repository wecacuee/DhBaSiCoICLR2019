
%\subsection{Future work}
%Items to improve the algorithm:
%\begin{itemize} \item
%\DONE{Justify the computational cost of constraint} The cost of going through the entire state space.
%How do you extend to a network? and large state spaces.
%\begin{enumerate}\item
%Observation 1: If there is only one goal, then the computation should not be any more than Q-learning.
%This can be accomplished by assuming that transitivity is satisfied till
%$\state_{t-1}$ and needs to be extend to only the next step. This sounds similar to the
% Floyd-Warshall dynamic programming update.
%However, this assumes that $\state_t$ is being visited for the first time.
%If the state $\state_t$ is being visited for the second time, the earlier
%value may be the shorter path for it.
%\end{enumerate}
%\item
%We need Q-value for exploration.
%\end{itemize}
